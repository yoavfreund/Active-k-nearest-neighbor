\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{natbib}

\def\R{{\mathbb{R}}}
\def\pr{{\rm Pr}}
\def\E{{\mathbb E}}
\def\X{{\mathcal X}}
\def\Y{{\mathcal Y}}
\def\H{{\mathcal H}}
\def\G{{\mathcal G}}
\def\B{{\mathcal B}}
\def\bias{{\rm bias}}
\def\supp{{\rm supp}}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{claim}[thm]{Claim}
\newtheorem{defn}[thm]{Definition}
\newtheorem{assump}{Assumption}
\newtheorem{open}{Open problem}
\newenvironment{proof}{\noindent {\sc Proof:}}{$\Box$ \medskip}

\DeclareMathOperator*{\argmax}{arg\,max}

\title{An active $k$-nearest neighbor algorithm}

\begin{document}

\maketitle

\section{Setup and goals}

Basic setup:
\begin{itemize}
\item Instance space $\X \subset \R^d$. Data are generated from a distribution $\mu$ on $\X$.
\item Label space $\Y = \{-1,+1\}$. Each $x \in \X$ is labeled according to conditional probability function $\eta(x) = \pr(Y=1|X=x)$.
\item Let $g^*$ denote the Bayes-optimal classifier on $\X$:
$$ g^*(x) = \mbox{sign}(2 \eta(x) - 1) .$$
\end{itemize}

\noindent
Goal: \emph{pool-based active learning}
\begin{itemize}
\item We have a finite dataset $X \subset \R^d$ of $n$ points, and we are able to query the labels of any of them.
\item Given parameters $0 < \epsilon, \delta < 1$, our goal is to query the labels of some points $Q \subset X$, and infer the labels of the remainder, so that with probability at least $1-\delta$, at most $\epsilon n$ of the inferred labels disagree with $g^*$.
\end{itemize}

\noindent
Alternative goal: \emph{stream-based active learning}
\begin{itemize}
\item Data arrive continuously, one point at a time. For each point, we decide whether or not to query its label.
\item The classifier $g_t$ at time $t$ is (some kind of) $k$-nearest neighbor classifier based on the points labeled so far. We require $g_t \rightarrow g^*$ and we consider the rate of convergence in terms of the number of labeled points.
\end{itemize}

\section{Notation}

For any $x \in X$ and any $p > 0$, let $r_p(x)$ denote the smallest radius $r$ such that $\mu(B(x, r)) \geq p$. 

For any $p, \gamma> 0$, let $\X_{p, \gamma}$ consist of all points $x \in \supp(\mu)$ such that:
\begin{itemize}
\item either $\eta(B(x,r)) \geq 1/2 + \gamma$ for all $0 \leq r \leq r_p(x)$ 
\item or $\eta(B(x,r)) \leq 1/2 - \gamma$ for all $0 \leq r \leq r_p(x)$.
\end{itemize}


\section{Algorithm for pool-based active learning}

Global variables:
\begin{itemize}
\item $U \subset X$, a subset of points deemed ``uncertain''
\item $Q \subset X$, points whose labels have been queried
\item $R \subset Q$, points in $Q$ that are random draws from the entire space $X$
\item $I \subset X$, points whose labels have been inferred, possibly incorrectly
\end{itemize}

We will use large deviation bounds for the class $\B$ of balls centered at points of $X$. This is a class of size at most ${n \choose 2}$. The bounds will assert the following: suppose $m$ points are drawn i.i.d. from $\mu|_S$, the restriction of $\mu$ to a subset $S \subset \X$, and labeled according to $\eta$. Then with probability at least $1-\delta$, the following properties hold for all $B \in \B$:
\begin{enumerate}
\item[(P1)] If $\mu|_S(B) \geq (k/m) \ln (n/\delta)$, then $B$ gets at least $k+1$ points.
\item[(P2)] If $B$ gets at least $k$ points, then the average of the labels in $B$ is equal to $\eta(B) \pm k^{-1/2}$.
\end{enumerate}
We will use several rounds of sampling. Let $G_t$ denote the good event that these conditions hold for the points drawn in round $t$.

\subsection{A single round of sampling}

\begin{figure}[h!]
\framebox{
\begin{minipage}[t]{6.3in}
{\bf function Extend}($k$, $p$, $\delta$):
\begin{itemize}
\item Setup:
\begin{itemize}
\item Define the region of uncertainty:
$$ U = X \setminus (Q \cup I) $$
\item Determine the sampling region:
$$ S =  \left( \bigcup_{x \in U} B(x, r_p(x)) \right) \cap X $$
\item Choose number of samples:
$$ m = \frac{|S|}{|X|} \cdot \frac{k}{p} \cdot \ln \frac{n}{\delta} $$
\end{itemize}

\item Querying:
\begin{itemize}
\item Let $M$ consist of $m$ points chosen uniformly at random from $S$
\item Let $M'$ consist of $m$ points chosen uniformly at random from $X$
\item For all $x \in (M \cup M') \setminus Q$: 
\begin{itemize}
\item Query $x$'s label
\item Add $x$ to $Q$
\end{itemize}
\item $R = R \cup M'$
\end{itemize}

\item Update:
\begin{itemize}
\item For all $x \in U$:
\begin{itemize}
\item Find the $k$ nearest neighbors of $x$ in $M$ (can probably do $M \cup R$)
\item If their labels have significant bias, in the sense that their average has absolute value $\geq k^{-1/2}$, label $x$ accordingly and add it to $I$
\end{itemize}
\item For all $x \in I$:
\begin{itemize}
\item Find the $k$ nearest neighbors of $x$ in $R$
\item If their labels have significant bias and disagree with $x$'s current label, move all of $B(x,r_p(x)) \cap (I \setminus \{x\})$ out of $I$ (not sure exactly what to do here)
\end{itemize}
\end{itemize}
\end{itemize}
\end{minipage}}
\caption{Extend: the key subroutine for pool-based active learning}
\label{fig:extend}
\end{figure}

\begin{lemma}
Consider round $t$ of sampling. Suppose event $G_t$ holds. For any $x$, let $\widetilde{r}(x)$ denote the smallest radius $r$ such that $B(x,r)$ contains $k+1$ points of $M$. Then for all $x \in U$,
\begin{enumerate}
\item[(a)] $\widetilde{r}(x) \leq r_p(x)$
\item[(b)] the bias of the $k$ nearest neighbors of $x$ in $M$ agrees with $\eta(B(x,\widetilde{r}(x))) \pm 1/k^{1/2}$.
\end{enumerate}
\label{lemma:bias-bound}
\end{lemma}
\begin{proof}
Let's begin with (a). Choose any $x \in U$ and write $B = B(x,r_p(x))$. The points in $M$ can be considered to be $m$ i.i.d. draws from $\mu|_S$. Ignoring the small discrepancy between $\mu(S)$ and $|S|/|X|$, we have 
$$ \mu|_S(B) = \frac{\mu(B)}{\mu(S)} \geq \frac{p}{\mu(S)} \geq \frac{k}{m} \ln \frac{n}{\delta}$$
and thus by (P1) $M$ includes at least $k+1$ points of $B$.

Part (b) then follows directly by appealing to (P2).
\end{proof}

\begin{thm}
Suppose that in round $t$, the starting uncertainty region is $U$. Assume event $G_t$ occurs. Then for all $x \in U$ that aren't queried, the following two properties hold.
\begin{enumerate}
\item[(a)] If $x \in \X_{p, 2k^{-1/2}}$, then by the end of the round, its label is inferred to be $g^*(x)$.
\item[(b)] If the label of $x$ is inferred but is not equal to $g^*(x)$, then $x \not\in \X_{p, k^{-1/2}}$.
\end{enumerate}
\end{thm}
\begin{proof}
Pick any $x \in U$ whose label is not queried by the end of round $t$. By Lemma~\ref{lemma:bias-bound}, its $k$ nearest neighbors in $M$ lie within distance $\widetilde{r}(x) \leq r_p(x)$, and its $k$ nearest neighbors have average label in the range $\eta(B(x, \widetilde{r}(x))) \pm k^{-1/2}$. 

For part (a), observe that by the definition of $\X_{p,\gamma}$, this average label has sign $g^*(x)$ and absolute value $> k^{-1/2}$. Thus the bias is significant and the label of $x$ is correctly inferred.

For part (b), we observe that if $x \in \X_{p, k^{-1/2}}$, then the average label of $x$'s $k$ nearest neighbors would have the correct sign, $g^*(x)$.
\end{proof}

\subsection{A simple active learning loop}

Here's a simple idea: keep $k$ fixed, with the goal of trying to correctly classify all $x \in \X_{p,\gamma}$ with $p > 1/n$ and $\gamma > 1/\sqrt{k}$. We won't quite be able to do this because of mind-changes.

\begin{itemize}
\item Given: $X$, $\gamma$, $\delta$
\item $k = 1/\gamma^2$
\item $Q = I = \emptyset$
\item for $t = 0, 1, \ldots, \log n$:
\begin{itemize}
\item $p_t = 1/c^t$
\item $\delta_t = \delta/(1 + \log n)$
\item Extend($k$, $p_t$, $\delta_t$)
\end{itemize}
\end{itemize}

Let's start by analyzing this in the case where there are no mind-changes.

\subsection{Analysis: no mind changes}

Fix $\gamma = k^{-1/2}$. For $\tilde{p} \sim 1/n$, define $\widetilde{\X}^+ = \X_{\tilde{p}, \gamma}$ and likewise $\widetilde{\X}^-$. Then $\widetilde{\X} = \widetilde{\X}^+ \cup \widetilde{\X}^-$ is exactly the set of points that could reasonably be expected to be correctly classified by a $k$-NN classifier with access to all the labels in $X$. We'd like to do almost as well after querying just a fraction of the labels.

\begin{enumerate}
\item {\it No mind changes.} We will assume no mind-changes: that is, for any $r>0$,
\begin{itemize}
\item $x \in \widetilde{\X}^+ \implies \eta(B(x,r)) > 1/2 - \gamma$
\item $x \in \widetilde{\X}^- \implies \eta(B(x,r)) < 1/2 + \gamma$
\end{itemize}
This means that no point in $\widetilde{\X}$ will ever be misclassified.

\item {\it After round $t$.} At the end of round $t$, every point in $\X_{p_t, \gamma}$ will either have been queried or will have had its label correctly inferred. What remains, at most, is then $\partial_{p_t, \gamma}$, where we define
$$ \partial_{p, \gamma} = \X \setminus \X_{p, \gamma} .$$
The sampling region in the next round is then $S_{t+1} \subset R_{t+1} \cap X$, where
$$ R_{t+1} = \bigcup_{x \in \partial_{p_t, \gamma}} B(x, r_{p_{t+1}}(x))  .$$

\item {\it Smoothness condition.} Recall our nearest neighbor-inspired variant of the Holder condition: $\eta$ is $(\alpha, L)$-smooth if for all $x \in \supp(\mu)$ and all $r > 0$,
$$ |\eta(B(x,r)) - \eta(x)| \leq L \mu(B^o(x,r))^\alpha .$$
In [CD14], it is shown that under this condition, for any $p, \gamma$, we have
$$ \partial_{p,\gamma} \cap \supp(\mu) \subset \{x: |\eta(x) - 1/2| \leq \gamma + Lp^\alpha\} .$$

Here's a stronger condition: for all $x \in \supp(\mu)$ and all $r > 0$,
$$ |\eta(x') - \eta(x)| \leq L \mu(B^o(x,r))^\alpha \ \ \mbox{for all $x' \in B(x,r)$}.$$
Under this condition, it easily follows that
$$ \bigcup_{x \in \partial_{p_1, \gamma}} B(x, r_{p_2}(x)) \ \subset \ \{x: |\eta(x) - 1/2| \leq \gamma + L(p_1^\alpha + p_2^\alpha)\} .$$
\item {\it Margin condition.} For $\beta > 0$, we say $(\mu, \eta)$ satisfies the $\beta$-margin condition if there exists a constant $C > 0$ such that for any $t$,
$$ \mu(\{x: |\eta(x) - 1/2| \leq t\}) \leq C t^\beta .$$

\item {\it Bounding the size of the sampling region.} Under the strong smoothness condition, we have
$$ R_{t+1} \subset  \{x: |\eta(x) - 1/2| 
\ \leq \ 
\gamma + L(p_t^\alpha + (p_t/c)^\alpha)\} .$$
Define $L' = L(1 + c^{-\alpha})$. Under the margin condition, we then get
$$ \mu(R_{t+1}) 
\ \leq \ 
C (\gamma + L' p_t^\alpha)^\beta 
\ \leq \ 
2C \cdot \max \left(\frac{1}{k^{1/2}}, \frac{L'}{c^{\alpha t}} \right)^\beta.
$$
Initially (when $t$ is small), the dominant term is $(1/c^{\alpha \beta})^t$. Once $t \geq (1/(2 \alpha)) \log_c k$, the dominant term becomes $1/k^{\beta/2}$.

\item {\it Label complexity analysis under optimal choice of $k$.} Under the smoothness and margin condition, the optimum setting of $k$ is $n^{2\alpha/(2\alpha+1)}$. 


\end{enumerate}



\begin{center}
\begin{huge}
Older material
\end{huge}
\end{center}

\pagebreak

\section{Setup}

Basic Transductive setup:
\begin{itemize}
\item Intance set $\X =\{x_1,\dots,x_n\}$, $x_i \in {\bf X}$, where
  ${\bf X}$ is a metric space. . The set
  $\X$ is fixed and known to the learner. The probability of each $x
  \in \R$ is $1/n$.
\item Label space $\Y = \{-1,+1\}$. Each $x \in \X$ is associated with
  a conditional probability function $\eta(x) = \pr(Y=1|X=x)$.
\item Let $g^*$ denote the Bayes-optimal classifier on $\X$:
  $$ g^*(x) = \mbox{sign}(2 \eta(x) - 1) .$$
  \item Label Queries: the algorithm can select any example $x \in
    \X$, as a response it gets a label in $\Y$ chosen acccording to
    $\eta(x)$.    
  \end{itemize}

\section{Notation}
\begin{itemize}
\item {\bf Balls:} For any $x \in \X$ an $r\geq0$,
  $B(x,r)$ denotes the subset of $\X$ inside ball of radius $r$ centered at $x$.
  \item {\bf Probability of a ball} The probability of a ball is the
    fraction of points in $\X$ that fall inside $B(x,r)$.
    $$ \mu(B(x,r)) = \frac{ |B(x,r)|}{n}$$
  \item {\bf conditional probability given a ball:}
    $$ \eta(B(x,r)) = \frac{\sum_{x \in B(x,r)}\eta(z)}{|B(x,r)|}$$
  \item {\bf measuring radius in terms of probability}
For any $x \in X$ and any $p > 0$, let $r_p(x)$ denote the smallest
radius $r$ such that $\mu(B(x, r)) \geq p$.
\end{itemize}

\newcommand{\NNB}{{\bf NNB}}
\newcommand{\ANNB}{{\bf ANNB}}
\newcommand{\ONNB}{{\bf oracle-NNB}}

\section{The near-neighborhoods algorithm}
The near neighborhood (\NNB) algorithm is a variant of AKNN where the balls
that are considered for the prediction of a point $x$ are {\em all} balls
the contain $x$ while in AKNN only balls that are centered at $x$ are
considered.

Suppose that we are given $n$ labeled examples chosen uniformly at
random from $\X$ and labeled according to $\eta(x)$.

Given that set of labels each ball is as $-,?,+$ according to the
condition used in AKNN. We say that a ball labeled $\pm$ is {\em
  determined}, a ball labeled $?$ is {\em undetermined}.

We define the partial order on balls that is defined by containment.

The labels of the balls induce a labeling on the points in $\X$. Each
point has one of {\em four} labels: $-1,+1,?,!$. Given a point $x \in
\X$ we define the {\em minimal set of determined balls for $x$},
denoted $\B(x)$, to be the set of determined balls that contain $x$
that is minimal (in terms of the containment partial order). We can
construct this set by starting with the set of all determined balls containing
$x$ and eliminating any ball that has a subset-ball that is also in
the set.

Given the set of balls $\B(x)$ the label associated with $x$
according to the following rules:
\begin{itemize}
\item If $\B(x)$ is empty the label is undetermined, or $?$.
\item If all of the balls in $\B(x)$ are labeled $+$, the label is $+1$.
\item If all of the balls in $\B(x)$ are labeled $-$, the label is $-1$
\item If $\B(x)$ contains both labels, then the label is {\em
    over-determined} or $!$. We will also refer to over-determined
  examples as the ``known unknown''.
\end{itemize}

We define the numerical prediction $\hat{y}$ to be $0$  if the label
is undetermined or overdetermined. We set it equal to the label otherwise

We define the error of the prediction as the produce
$-\hat{y}{y}$. Note that the error can be $-1,0,+1$. 

\NNB\ is a passive learning algorithm, in other words the queriy
points are selected IID from $\X$.

\begin{claim}
  The algorithm \NNB\ converges to Bayes rule as the number of
  labeled examples increases to $\infty$.
  \end{claim}

\section{Enforcing locality}
For technical reasons, we consider balls that are smaller and smaller as the algorithm progresses. Let
$1=p_1 >p_2>\cdots >p_k=1/n$ be the radius probabilities. Then in iteration $i$ we restrict the set of balls to be such that $\mu(B(x,r)) \leq p_i$.

\section{Active \NNB}

The main idea behind active \NNB\, or \ANNB\, is to concentrate the
queries on the known unknown. However, this is not done to the
exclusion of the other parts of the space, as any example can have an
unbounded number of mind changes. In other words, an example whose label at some point is $+1$
can be labeled $-1$ later on and vice versa. (These are the ``unknown
unknowns'').

\ANNB\ operates in phases. At the start of phase $t$ a concentration
area $C_t \subset \X$ is computed. Then, $2n$ queries are are
made. There are $n$ {\em passive} queries and $n$ {\em active} queries.
The passive queries are selected uniformly at random from $\X$, the
active quries are selected uniformly at random from $C_t$.

After the answers to the queries are recieved, they are used to update
the counter $k_+,k_-$ in the balls. Passive points are used in each
ball that contains them. Active points are a bit more subtle, they are
used only in balls that are conained in $C_t$. In other words if a
ball contains an element $x$ that is {\em not} in $C_t$ then this ball
cannot use {\em any} of the active samples for epoch $t$.

Based on the additional queries, the label $\pm +1,?$ for each ball is
updated, and based on that the label of each $x \in \X$ is updated.

Finally, $C_{t+1}$ is calculated as follows. Let $U_t$ be the set $x
\in \X$ such that the label of $x$ is overdetermined, or ``!''.  $C_{t+1}$
is defined as the union of the minimal sets of determined balls for $x
\in U_t$, In other words:
\[
  C_{t+1} = \bigcup \left\{\B(x,r) | x \in U_t, \mu(\B(x,r)) \leq p_t \right\}
  \]

  \section{A comparator algorithm}
  In order to bound the query complexity of \ANNB\ we consider a
  variant of \ANNB\ that is aided by and oracle. Intuitively, the
  oracle removes the need for a passive sample by informing the
  algorithm that the prediction on a point $x$ is ``final''. I.e. it
  will not change as the number of queries increases to infinity.

  For this algorithm, which we denote by \ONNB\, we can more easily
  give bounds. As this framework is local, our goal is to provide
  point-wise bounds.

  \ONNB\ differs from \ANNB\ in two important ways
  \begin{enumerate}
  \item It makes only active queries, not passive queries.
  \item The set $U_t$ contains, in addition to overdetermined
    points. The points where the label is $+1$ or $-1$ but the oracle
    informs the algorithm that there will be mind cahnges on $x$ in
    the future. 
  \end{enumerate}

  Intuitively, \ONNB\ contines to collect information on $x$ until the
  label of $x$ converges, at which point it does not need more
  samples for $\B(x)$ so it stop collecting this information.

  \section{Pointwise analysis}

  Fix a point $x \in \X$ and compare the behaviour of \NNB\ and \ONNB\
  on $x$.

  Let the support set for $x$ be th set of queries that influence the predicted label for $x$. In other words removing the examples not in the support does not effect change the way the algorithm unfolds wrt that point.

  {\bf Can we prove that } The size of the support for any example using \ONNB\ is at least half of the size of the support using \ANNB\
    
\pagebreak

\section{Setup}

\begin{enumerate}
\item Setting: {\it Binary classification.}

\begin{itemize}
\item Instance space $\X \subset \R^d$. Data points $x$ are generated from a distribution $\mu$ on $\X$.
\item Label space $\Y = \{-1,+1\}$. Each $x \in \X$ is labeled according to conditional probability function $\eta(x) = \pr(Y=1|X=x)$.
\item The Bayes-optimal classifier on $\X$ is $g^*(x) = \mbox{sign}(2 \eta(x) - 1)$.
\end{itemize}

\item Goal: {\it Pool-based active learning.}

\begin{itemize}
\item We have a finite dataset $X \subset \R^d$ of $n$ points, and we are able to query the labels of any of them.
\item Given parameters $0 < \epsilon, \delta < 1$, our goal is to query the labels of some of the points, and infer the labels of the remainder, so that with probability at least $1-\delta$, at most $\epsilon n$ of the inferred labels disagree with the Bayes-optimal $g^*$.
\end{itemize}

\end{enumerate}


\section{An active learning algorithm}

\begin{enumerate}

\item {\it Algorithm overview.}

There are multiple rounds of labeling, and during each, query points are chosen at random from the current ``uncertainty region''.

\item {\it Sampling balls.}

There is a set of ``balls'' $\B$, balls of different sizes that can get arbitrarily small. These are the balls on which we assess label bias. 

We query points that are chosen at random, under two types of sampling. 
\begin{itemize}
\item Background random sampling of the entire data set $X$.
\item Focused random sampling of a subset of $X$.
\end{itemize}
For each $B \in \B$, we know which of the labels seen so far can be considered random samples from $B$. On that basis, we assign a {\it provisional label}
$$ \yh(B) = 
\left\{
\begin{array}{ll}
+ & \mbox{significant $+$ bias} \\
- & \mbox{significant $-$ bias} \\
? & \mbox{otherwise}
\end{array}
\right.
$$

\item {\it $\Delta$-accurate bias estimates.}

The bias estimator that we use returns $\{+,-,?\}$ for any ball $B$ and has the following property that holds (with high probability) uniformly over all balls $B$ under consideration:
\begin{itemize}
\item If $\eta(B) \geq 1/2 + 2 \Delta$, it returns $+$.
\item If $\eta(B) \leq 1/2$, it does not return $+$.
\item Likewise with $-$.
\end{itemize}
Roughly speaking, this requires having $1/\Delta^2$ random labeled points in each ball.

Over time, as more random samples appear in any given ball $B$, it is theoretically possible that this provisional label would change from $+$ to $-$ or vice versa. In such cases, our large deviation bounds allow us to retain either label.

\item {\it Probability radius.}

For any $x \in \X$ and any $p > 0$, let $r_p(x)$ denote the smallest radius $r$ such that $\mu(B(x, r)) \geq p$. 

\item {\it Balls used to evaluate a given point.}

For any $x \in \X$, let $\B_x \subset \B$ denote the collection of balls that may be used in determining $x$'s label. This could be, for instance,
\begin{itemize}
\item[(a)] all balls containing $x$, or
\item[(b)] all balls centered at $x$.
\end{itemize}

\item {\it Regions at a given probability scale.}

For any point $x \in \X$ and value $p > 0$, let $\B_{x,p}$ be a subset of $\B_x$ that is considered to be at ``probability-scale $p$''. For instance, we could define
$$ \B_{x,p} = \{B \in \B_x: B \subseteq B(x,r_p(x))\},$$
but we leave it flexible for now. 

\noindent
{\it Assumption:} For any $x \in \X$ and $p > 0$, there exists some $B \in \B_{x,p}$ with $\mu(B) \geq c_o p$. Here $c_o > 0$ is an absolute constant.

\item {\it Provisional labels for data points.}

At any given time, at scale $p$, we will assign $x$ a provisional label
$$ \yh_p(x) = 
\left\{
\begin{array}{ll}
? & \mbox{if all $B \in \B_{x,p}$ have provisional label $\yh(B) = \ ?$} \\
+ & \mbox{if there is a $+$ ball and the rest are $+$ or $?$} \\ 
- & \mbox{if there is a $-$ ball and the rest are $-$ or $?$} \\ 
! & \mbox{if there is both a $+$ ball and a $-$ ball}
\end{array}
\right.
$$
Intuitively, the points we will deem uncertain are those that are labeled $!$ or $?$.

\item {\it The effective decision boundary.}

The true decision boundary is the set $\{x: \eta(x) = 1/2\}$, but this doesn't tell us which points we can reasonably hope to correctly label with a finite sample.

For any $p, \gamma > 0$, let $\X_{p, \gamma}^+$ consist of all points $x \in \supp(\mu)$ such that:
\begin{itemize}
\item $\eta(x) \geq 1/2 + \gamma$
\item $\eta(B) \geq 1/2$ for all $B \in \B_{x,p}$
\item $\eta(B) \geq 1/2 + \gamma$ for some $B \in \B_{x,p}$ with $\mu(B) \geq c_o p$
\end{itemize}
We can define $\X_{p,\gamma}^-$ similarly, and $\X_{p,\gamma} = \X_{p,\gamma}^+ \cup \X_{p,\gamma}^-$. The {\it effective boundary} then consists of all remaining points,
$$ \partial_{p,\gamma} = \X \setminus \X_{p,\gamma} .$$

\end{enumerate}

{\bf Yoav:} Might be possible to prove an upper bound on the number of queries if the probability of the boundary $\partial_{p,\gamma}$ is small.

\subsection{One round of sampling}

The algorithm proceeds in rounds, each with an equal amount of {\it background} sampling and {\it focused} sampling. At the beginning of round $t$:
\begin{itemize}
\item $Q_t \subseteq X$ is the set of points queried so far. 

Each point $x \in Q_t$ has accompanying information that tells us, for any $B \in \B$, whether $x$ can be considered a random sample from $B$.

\item $I_t \subseteq X$ is the set of points whose labels were inferred.

\item $U_t \subseteq X$ is the set of points deemed ``uncertain'' according to some criterion.
\end{itemize}
Based on these, the focused sampling region for round $t$ is
$$ S_t = \bigcup_{x \in U_t} \bigcup_{B \in \B_{x, p_t}} B .$$
Here $(p_t)$ is a predefined sequence with $p_t \downarrow 0$.

\begin{figure}[h!]
\framebox{
\begin{minipage}[t]{6.3in}
{\bf function Extend}($k$, $p$, $\delta$):

\vspace{.1in}
Global variables: $Q, I, U$.

\vspace{.1in}
Setup:
\begin{itemize}
\item Sampling region:
$$ S =  \left( \bigcup_{x \in U} \bigcup_{B \in \B_{x,p}} B \right) \cap X $$
\item Choose number of samples:
  $$ m = \frac{|S|}{|X|} \cdot \frac{k + c_1 \sqrt{k}}{c_o p} $$
\end{itemize}

\vspace{.1in}
Querying:
\begin{itemize}
\item Let $M$ consist of $m$ points chosen uniformly at random from $S$
\item Query the labels of all $x \in M \setminus Q$ and add these points to $Q$
\end{itemize}

\vspace{.1in}
Infer labels using $M$:
\begin{itemize}
\item For all $B \in \{B \in \B_{x,p}: x \in U\}$: Use $M \cap B$ to set $\yh(B)$
\item For all $x \in U \setminus Q$:
\begin{itemize}
\item Use $\{\yh(B): B \in \B_{x,p}\}$ to set $\yh_{p}(x)$
\item If $\yh_{p}(x) \in \{+,-\}$ and $\mbox{\tt oracle-check}(x,p)$: Add $x$ to $I$ with the given label
\end{itemize}
\end{itemize}

\vspace{.1in}
Update uncertainty region: 
\begin{itemize}
\item $U = U \setminus (Q \cup I)$
\end{itemize}

\end{minipage}}
\caption{Subroutine for active learning given {\tt oracle-check}.}
\label{fig:extend}
\end{figure}

\subsection{Large deviation bounds}

We will use large deviation bounds for the class $\B$ of regions. 
\begin{lemma}
Choose any $0 < \delta < 1$ and define $C_\delta = \ln (4 |\B|/\delta)$. There are universal constants $c',c''>0$ for which the following is true. If $m$ points are drawn i.i.d. from $\mu|_S$, the restriction of $\mu$ to a subset $S \subset \X$, and labeled according to $\eta$, then with probability at least $1-\delta$, the following properties hold for all $B \in \B$:
\begin{enumerate}
\item[(a)] If $k \geq C_\delta$ and $\mu|_S(B) \geq (k + c' \sqrt{k C_\delta})/m$, then $B$ gets at least $k$ points.
\item[(b)] If $B \subseteq S$ gets at least $k$ points, then the empirical bias of the labels in $B$ is equal to $\eta(B) \pm c'' \sqrt{C_\delta/k}$.
\end{enumerate}
\label{lemma:bias-estimates}
\end{lemma}
\begin{proof}
These properties follow immediately from Lemmas~\ref{lemma:points-in-balls-finite} and \ref{lemma:bias}.
\end{proof}

Let $G_t$ denote the good event that these conditions hold for the points drawn in round $t$ of sampling, and let $G_{\leq t} = G_1 \cap G_2 \cap \cdots \cap G_t$.

In what follows, we will take $\delta > 0$ to be a constant and will assume that $k \geq C_\delta$. We also define $c_1 = c' \sqrt{C_\delta}$ and $c_2 = c'' \sqrt{C_\delta}$. Finally, let
$$ \Delta_k = \frac{c_2}{k^{1/2}}.$$

Our bias-estimates are computed as follows. On any round of the algorithm, let $S \subseteq \X$ denote the sampling region and $M$ the randomly chosen samples in this region. For any region $B \subseteq S$ under consideration, 
$$ \yh(B) = 
\left\{
\begin{array}{ll}
+ & \mbox{if $|B \cap M| \geq k$ and empirical bias $\geq 1/2 + \Delta_k$} \\
- & \mbox{if $|B \cap M| \geq k$ and empirical bias $\leq 1/2 - \Delta_k$} \\
? & \mbox{otherwise}
\end{array}
\right.
$$
The key property we will need for these estimates is that they are $\Delta_k$-accurate in the following sense.
\begin{defn}
Pick any $\Delta > 0$. We say bias-estimates $\yh(B) \in \{+,-,?\}$ are $\Delta$-accurate if they satisfy the following properties:
\begin{itemize}
\item If $\eta(B) \geq 1/2 + 2\Delta$, then $\yh(B) = +$.
\item If $\eta(B) \leq 1/2 - 2\Delta$, then $\yh(B) = -$.
\item If $\yh(B) = +$ then $\eta(B) > 1/2$.
\item If $\yh(B) = -$ then $\eta(B) < 1/2$.
\end{itemize}
\label{defn:delta-accurate}
\end{defn}

\begin{lemma}
Consider round $t$ of sampling, with uncertainty set $U$ and parameters $k,p$. Suppose event $G_t$ holds. Then for all $x \in U$, the bias-estimates $\{\yh(B): B \in \B_{x,p}\}$ are $\Delta_k$-accurate.
\label{lemma:accurate-bias}
\end{lemma}
\begin{proof}
This follows immediately from Lemma~\ref{lemma:bias-estimates}.
\end{proof}

\begin{thm}
Suppose that in round $t$, the starting uncertainty region is $U$. Assume event $G_{\leq t}$ occurs. Then for all $x \in U$ that aren't queried, the following two properties hold.
\begin{enumerate}
\item[(a)] If $x \in \X_{p,2\Delta_k}$, then by the end of the round, its label is inferred to be $\yh_{p}(x) = g^*(x)$.
\item[(b)] If $\yh_{p}(x) \in \{+,-\}$ but is not equal to $g^*(x)$, then $x \not\in \X_{p,0}$.
\end{enumerate}
\end{thm}
\begin{proof}
Pick any $x \in U$ whose label isn't queried by the end of round $t$. Without loss of generality $\eta(x) \geq 1/2$.

For part (a), if $x \in \X_{p,2\Delta_k}$, then every $B \in \B_{x,p}$ has $\eta(B) \geq 1/2$ and thus, by Lemma~\ref{lemma:accurate-bias}, gets $\yh(B) \in \{+,?\}$ in any round of sampling. Moreover, by assumption there is some $B' \in \B_{x,p}$ for which $\mu(B') \geq c_o p$ and $\eta(B') \geq 1/2 + 2\Delta_k$. Letting $S$ denote the sampling region in round $t$ and ignoring the small discrepancy between $\mu(S)$ and $|S|/|X|$,
$$ \mu|_S(B') = \frac{\mu(B')}{\mu(S)} \geq \frac{c_o p}{\mu(S)} \geq \frac{k + c_1 \sqrt{k}}{m}$$
and thus by Lemma~\ref{lemma:bias-estimates}(a) we have $|M \cap B'| \geq k$. Thus $\yh(B)$ would be assigned $+$ in round $t$, if not earlier, whereupon $\yh_p(x) = +$.

For part (b), suppose $\yh_{p}(x) = -$. Then some $B \in \B_{x,p}$ has $\yh(B) = -$, implying by Lemma~\ref{lemma:accurate-bias} that $\eta(B) < 1/2$. This can only be happen if $x \not\in \X_{p,0}$. 
\end{proof}

The Algorithm in Figure~\ref{fig:extend} makes use of an external subroutine called {\tt oracle-check}. This has the following semantics: $\mbox{\tt oracle-check}(x,p)$ returns {\tt true} if $x \in \X_{p,\gamma}$ and {\tt false} otherwise.

\begin{cor}
\item Pick any round $t$, and say parameters $k,p$ are used in that round. Suppose that $G_{\leq t}$ holds and that $\gamma \leq 2\Delta_k$. Let $U$ denote the uncertainty set at the start of the round. During round $t$, all points in $U \cap \X_{p,2\Delta_k}$ will be placed in $I$. Moreover, assuming the correctness of the {\tt oracle-check} subroutine, any point placed in $I$ will be labeled correctly.
\end{cor}

\subsection{An active learning loop (using the oracle)}

Here's a simple idea: keep $k$ fixed, with the goal of trying to correctly classify all $x \in \X_{p,\gamma}$ with $p > 1/n$ and $\gamma = \Delta_k$. 

\begin{itemize}
\item Given: $X$, $\gamma$, $\delta$
\item Choose $k$ so that $\Delta_k = \gamma$ (i.e., $k = 4c_2^2/\gamma^2$)
\item $Q = I = \emptyset$
\item for $t = 0, 1, \ldots, T$:
\begin{itemize}
\item $p_t = 1/c^t$ for some $c > 1$
\item $\delta = \delta_o/(T + 1)$
\item Extend($k$, $p_t$, $\delta$)
\end{itemize}
\end{itemize}

Pieces of the analysis:
\begin{enumerate}
\item {\it One useful instantiation of $\B_x$.}

Suppose we define $\B_x$ to be all balls centered at $x$, and $\B_{x,p}$ to be all such balls with radius at most $r_p(x)$. In our earlier work we found that in this setting, in order to get good labels for $\X_{p,\gamma}$, 
$$ n \sim \frac{1}{p \gamma^2}$$
labels are necessary and sufficient. This is our point of reference.

\item {\it Invariants after round $t$.} 

At the end of round $t$, every point in $\X_{p_t,\gamma}$ will either have been queried or will have had its label correctly inferred. What remains, at most, is then $\partial_{p_t,\gamma}$. The sampling region in the next round is then a subset of $R_{t+1} = \partial_{p_t, p_{t+1}, \gamma}$, where we define
$$ \partial_{p,q,\gamma} = \bigcup_{x \in \partial_{p,\gamma}} \bigcup_{B \in \B_{x,q}} B .$$

\item {\it Label complexity: high level insights.}

The total number of queries is upper-bounded by, roughly,
$$ \sum_{t=0}^T \mu(R_t) \frac{k_t}{p_t} .$$
If we keep $k$ fixed at $1/\gamma^2$ and use geometrically decreasing $p_t$, this is at most $1/(p_T \gamma^2)$, even if $\mu(R_t)$ remains stuck at 1. This is exactly the label complexity of passive learning for $\X_{p_T, \gamma}$. So the benefit of active sampling comes from the shrinking of $\mu(R_t)$.

\end{enumerate}

\subsection{Label complexity under smoothness assumptions}

\begin{enumerate}

\item {\it A further assumption on $\B_{x,p}$.}

In this section we will impose the further condition that $B \in \B_{x,p} \implies B \subseteq B(x, r_p(x))$. As a consequence,
$$ \partial_{p,q,\gamma} \subseteq \bigcup_{x \in \partial_{p,\gamma}} B(x, r_q(x)) .$$

\item {\it Smoothness and margin conditions.}

\begin{itemize}
\item Notion of margin: for any $t \in (0, 1/2)$, define the {\it $\tau$-width margin} to be
$$ M(\tau) = \{x \in \X: |\eta(x) - 1/2| \leq \tau\} .$$
\item Smoothness: we define a notion of $(\alpha,L)$-smoothness condition under which, for any $p,q > 0$,
$$ \bigcup_{x \in \partial_p} B(x, r_q(x)) \subseteq M(\gamma + L(p^\alpha + q^\alpha)) .$$
\item Tsybakov margin condition: there is $C > 0$ and $\beta > 0$ for which
$$ \mu(M(\tau)) \leq C \tau^\beta.$$
\end{itemize}

\item {\it Bounding the size of the sampling region.}

Under the smoothness condition, we have by Lemma~\ref{lemma:boundary-plus} that
$$ R_{t+1} \subseteq M(\gamma + L(p_t^\alpha + (p_t/c)^\alpha)) .$$
Define $L' = L(1 + c^{-\alpha})$. Under the margin condition, we then get
$$ \mu(R_{t+1}) 
\ \leq \ 
C (\gamma + L' p_t^\alpha)^\beta 
\ \leq \ 
2C \cdot \max \left(\frac{1}{k^{1/2}}, \frac{L'}{c^{\alpha t}} \right)^\beta.
$$
Define $t_o = (1/(2 \alpha)) \log_c k$. 
\begin{itemize}
\item When $t \leq t_o$, the dominant term is $(1/c^{\alpha \beta})^t$, which is exponentially shrinking, as we'd like.
\item When $t > t_o$, the dominant term becomes $1/k^{\beta/2}$, which is constant.
\end{itemize}
During the second regime, there is no benefit to active learning. So we should stop when $t = t_o$.

\item {\it Label complexity: more detail.}

Use $p_t = 1/2^t$ and stop after $T = (1/(2 \alpha)) \lg k$ rounds. Then the label complexity is, roughly,
$$ \sum_{t=0}^T \mu(R_t) \frac{k}{p_t} = k \sum_{t=0}^T \frac{2^t}{2^{\alpha\beta t}} = k \sum_{t = 0}^T \left( \frac{1}{2^{\alpha\beta - 1}}\right)^t .$$
Consider three cases for the geometric series $(1/2^{\alpha\beta - 1})^t$.
\begin{itemize}
\item $\alpha \beta > 1$. Then the geometric series has ratio $< 1$ and the label complexity is $k$.
\item $\alpha \beta = 1$. The geometric series has ratio 1 and the label complexity is $k T \sim k \lg k$.
\item $\alpha \beta < 1$. The geometric series has ratio $> 1$ and the label complexity is $k 2^{(1-\alpha\beta)T} = k^{(2\alpha + 1 - \alpha\beta)/(2 \alpha)}$.
\end{itemize}
This correctly classifies $\X_{p_T, \gamma}$, for which passive sampling would need roughly 
$$ \frac{1}{p_T^2 \gamma^2} = k2^T = k^{(2\alpha + 1)/(2 \alpha)}$$
samples. If we think of this as $n$, then we should set $k = n^{2\alpha/(2\alpha+1)}$.
Putting it all together, we get the following label complexity bounds (compared to $n$):
$$ 
\begin{array}{ll}
n^{2\alpha/(2\alpha+1)} & \mbox{if $\alpha \beta > 1$} \\
n^{2\alpha/(2\alpha+1)} \log n & \mbox{if $\alpha \beta = 1$} \\
n^{(2\alpha + 1 - \alpha\beta)/(2\alpha+1)} & \mbox{if $\alpha \beta < 1$}
\end{array}
$$

\end{enumerate}

\section{Deterministic characterizations}

The goal of this section is to characterize the conditional distribution of the labels given a point using quantities that control the random quantities define earlier. The general plan is to use these quantities in guarantees regarding the query complexity of the algorithm. Note that the sample space $\X$ is fixed and know. The unknown quaantity is $\eta(x)$

\begin{enumerate}

\item {\it Sampling regions.}

There is a set of ``balls'' $\B$, regions of different sizes that can be as small as a single point in $\X$. These are the atomic regions on which we assess label bias. 

\item {\it bias of balls} For each $B \in \B$ we define the bias $B$ as 
$$ \eta(B) = \frac{1}{|B|} \sum_{x \in B}\eta(x) $$
\item {\it thresholding}
We define a provisional label for a ball:
$$\yh(B) =  
\begin{cases}
+ & \mbox{ if } \eta(B)>\gamma \\
? & \mbox{ if } |\eta(B)| \leq \gamma \\
- & \mbox{ if } \eta(B)<-\gamma 
\end{cases}
$$

\item {\it Probability radius.}

For any $x \in \X$ and any $p > 0$, let $r_p(x)$ denote the smallest radius $r$ such that $\mu(B(x, r)) \geq p$. 
In other words $\mu(B) = |B| / |\X|$

\item {\it Regions at a given probability scale.}

For any point $x \in \X$ and value $p > 0$, let $\B_{x,p}$ be a subset of $\B_x$ that is considered to be at ``probability-scale $p$''. For instance, we could define
$$ \B_{x,p} = \{B | x \in B, p\leq \mu(B) \leq 2p\}$$

\item {\it Provisional labels for data points.}

given a scale $p$ and a conditional probability threshold $\gamma$, we  assign $x \in \X$ a provisional label
$$ \yh_{p,\gamma}(x) = 
\left\{
\begin{array}{ll}
? & \mbox{if all $B \in \B_{x,p}$ have provisional label $\yh(B) = \ ?$} \\
+ & \mbox{if there is a $+$ ball and the rest are $+$ or $?$} \\ 
- & \mbox{if there is a $-$ ball and the rest are $-$ or $?$} \\ 
! & \mbox{if there is both a $+$ ball and a $-$ ball}
\end{array}
\right.
$$

\item{\it mind changes in $[0,1]$}
We fix $\gamma>0$ and $x$, $s(x,p) = \yh_{p,\gamma}(x)$ defines a maps $p \in [0,1]$ to $(!,-,+,?)$.
$s$ partitions the segment $[0,1]$ to sub-segments with constant labels. The segment that contains $p=0$ is the convergence, or, no-more-mind-changes segments. The segment above it is the final $!$ segment. My hypothesis is that our algorithm relies on the uniform sampling half for all segments but the last 2, and it uses the active region sampling for the final $!$ sampling until it reaches convergence. (it does not know that it reached convergence, therefor it has to continue uniform sampling.)


\item {\it The effective decision boundary.}
The true decision boundary is the set $\{x: \eta(x) = 1/2\}$, but this doesn't tell us which points we can reasonably hope to correctly label with a finite sample.

The effective decision boundary for a given level $\gamma$ and a radius $p$ is the subset of $x \in \X$ for which 
$p$ is not in the final convergence segment.

\end{enumerate}

\section{Another way to write the algorithm}
\label{sec:alg-2}

\subsection{Key ideas}

\begin{enumerate}

\item {\it Pool of unlabeled points.} 

We begin with a pool $X$ of $n$ points drawn at random from an unknown underlying distribution $\mu$ on the instance space $\X$. We are allowed to query the label of any $x \in X$; this label follows the conditional probability distribution
$$ \pr(Y = 1|X = x) = \eta(x)$$
where $\eta$ is also unknown.

\item {\it Poisson sampling.} 

We adopt a slightly unusual sampling process.
\begin{itemize}
\item Typical sampling scheme: Choose $m'$ points at random from a given sampling region $S$ of size $m$.
\item What we do: Pick each $x \in S$ with probability $m'/m$, independently.
\end{itemize}
This permits a cleaner implementation and analysis, as we now outline. 

Every point $x \in X$ has its own random value $T_x$ chosen independently from the uniform distribution over $[0,1]$. If we merely wanted to query uniform-random points from $X$, we could achieve this by choosing points in increasing order of their $T_x$ values. This can be visualized as having a ``knob'' $\tau$ that begins at zero and is gradually increased; at any given time, the queried points are then precisely those with $T_x \leq \tau$.

For active learning, will have a combination of
\begin{itemize}
\item Background random sampling of the entire data set $X$.
\item Focused random sampling of subsets of $X$.
\end{itemize}
We will do this by having several knobs: a background sampling knob that works as described above, and other, higher-valued, knobs that apply to different subsets of $X$ where we want denser sampling.

\item {\it Sampling regions.}

There is a set of ``balls'' $\B$, regions of different sizes that can get arbitrarily small. These are the regions on which we assess label bias. 

\begin{enumerate}

\item[(a)] {\it Random samples within a region.} The construction of $\B$ may or may not depend on the unlabeled points $X$, and this distinction determines which points can be considered {\it random draws} from a particular $B \in \B$.  

For instance, consider these two alternatives for defining $\B$:
\begin{itemize}
\item $\B$ consists of a pre-defined set of regions.
\item $\B$ consists of balls defined by pairs of points in $X$, with each $x,x' \in X$ yielding the ball $B(x,\|x-x'\|)$.
\end{itemize}
In the first case, all points $X \cap B$ are random draws from $\mu|_B$, the restriction of $\mu$ to region $B$. In the second case, this is also true except for the two points $x,x'$ that define the ball.

Let $X_B$ denote the points in $X$ that can be considered random draws from $\mu|_B$; this will typically be almost all of $B \cap X$. 

\item[(b)] {\it Query-set for a region.} For each region $B$, we define its {\it query-set} as the $O(k)$ points in it with lowest $T_x$ values. More precisely, we have a threshold $\tau(B) \in [0,1]$  associated with $B$, and the query-set for $B$ consists of 
$$\{x \in X_B: T_x \leq \tau(B)\} .$$ 
The threshold $\tau(B)$ will be roughly $O(k/|X_B|)$.

\item[(c)] {\it Processed regions.} When the labels of $B$'s query-set have all been queried, we say {\it $B$ has been processed}. We then use the labels of these points to assign a label to the region as a whole:
$$ \yh(B) = 
\left\{
\begin{array}{ll}
+ & \mbox{significant $+$ bias} \\
- & \mbox{significant $-$ bias} \\
? & \mbox{otherwise}
\end{array}
\right.
$$

\item[(d)] {\it Regions at different sampling scales.} Regions $B$ that are large---in the sense of containing many points---have low thresholds $\tau(B)$ and are thus likely to get processed earlier, modulo differences in the density of focused sampling across $X$.

We will group regions $B$ according to intervals of $[0,1]$ in which their thresholds $\tau(B)$ lie. Define a sequence 
$$ 0 < \tau_0 < \tau_1 < \ldots < \tau_{\ell_o} = 1, $$
for some positive integer $\ell_o$. Each $0 \leq \ell \leq \ell_o$ denotes a ``level of sampling''. A region $B$ belongs to the smallest level $\ell$ such that $\tau(B) \leq \tau_\ell$; let $\B_\ell$ be the collection of all such regions.

Thus $\B_0, \B_1, \ldots$ is a partition of $\B$ by sampling level, with $\B_0$ consisting of very large regions and subsequent $\B_1, \B_2, \ldots$ consisting of successively smaller regions that may not need to be considered if they are far from the decision boundary.

\end{enumerate}

\item {\it Balls used to evaluate a given point.}

For any $x \in \X$, let $\B(x) \subset \B$ denote the collection of balls that may be used in determining $x$'s label. This could be, for instance,
\begin{itemize}
\item all balls containing $x$, or
\item all balls centered at $x$.
\end{itemize}
As before, we can partition these regions by sampling-level, so that $\B_\ell(x) = \B(x) \cap \B_\ell$.

\begin{enumerate}
\item[(a)] {\it Provisional label at a given level.} The provisional label of $x$ at a given sampling level $\ell$ is a function of the labels $\yh(B)$ of regions $B \in \B_\ell(x)$. It is defined as follows:
\begin{equation}
\yh_\ell(x) = 
\left\{
\begin{array}{ll}
? & \mbox{if all $B \in \B_\ell(x)$ have $\yh(B) = \ ?$} \\
+ & \mbox{if there is a $+$ ball and the rest are $+$ or $?$} \\ 
- & \mbox{if there is a $-$ ball and the rest are $-$ or $?$} \\ 
! & \mbox{if there is both a $+$ ball and a $-$ ball}
\end{array}
\right.
\label{eq:provisional-label}
\end{equation}
Points labeled $!$ are ``known unknowns'': they are near the decision boundary and need further investigation. Points labeled $?$ are also uncertain, and will be treated as such.

\item[(b)] {\it Point processed at level $\ell$.} When $\yh_\ell(x)$ becomes known, we say ``$x$ has been processed at level $\ell$''. In the case $\yh_\ell(x) = \ !$, this could happen before some of the individual regions $B \in \B_\ell(x)$ have been processed.

\item[(c)] Let $L(x) \in \{0,1,\ldots,\ell_o+1\}$ be the first level at which $x$ has {\it not} yet been processed. Initially, all $L(x) = 0$.

At any given time, the density of sampling will vary across $X$. It might be high at points known to be near the decision boundary, low near points believed to be far from the boundary, and intermediate at points which were once believed to be far from the boundary but have recently be found to possibly be close to the boundary.

\end{enumerate}

\item {\it Uncertainty region.} At any given time, the focused sampling takes place in the neighborhood of an ``uncertainty region'' $U \subset X$. We can partition this region by level: $U_\ell = \{x \in U: L(x) = \ell\}$.

Initially, $U = X$.

\end{enumerate}


\subsection{One round of sampling}

The algorithm proceeds in rounds, each with an equal amount of {\it background} sampling and {\it focused} sampling. At the beginning of a round:
\begin{itemize}
\item $Q \subseteq X$ is the set of points queried so far. 

\item $U \subseteq X \setminus Q$ is the ``uncertain'' region. We can partition it by level: $U_\ell = \{x \in U: L(x) = \ell\}$.
\end{itemize}
The focused sampling for the round occurs around the {\it lowest-numbered non-empty uncertainty set} $U_\ell$.

\begin{figure}[h!]
\framebox{
\begin{minipage}[t]{6.4in}
{\bf function One-Round}

\vspace{.1in}
Global variables: 
\begin{itemize}
\item $Q$ - points queried so far.
\item $U$ - uncertain region. Can be partitioned by level: $U_\ell = \{x \in U: L(x) = \ell\}$
\end{itemize}

\vspace{.1in}
{\it Focused sampling.}
\begin{itemize}
\item Find the smallest $0 \leq \ell \leq \ell_o$ such that $U_\ell \neq \emptyset$. If there is no such $\ell$, move to the next step.
\item Query region:
$$ S =  \bigcup_{x \in U_\ell} \bigcup_{B \in \B_\ell(x)} \{z \in X_B: T_z \leq \tau(B)\} $$
\item Query the next point in $S \setminus Q$, ordered by $T_z$ values, and add to $Q$
\end{itemize}

\vspace{.1in}
{\it Background sampling.}
\begin{itemize}
\item Query the next point in $X \setminus Q$, ordered by $T_z$ values, and add to $Q$
\end{itemize}

\vspace{.1in}
{\it Processing points and regions.}
\begin{itemize}
\item For any region $B$ that is now processed: compute $\yh(B)$
\item For any point $x$ that is now processed at level $L(x)$:
\begin{itemize}
\item Determine $\yh_\ell(x)$ using (\ref{eq:provisional-label})
\item If $\yh_\ell(x) \in \{+,-\}$: set $U = U \setminus \{x\}$
\item If $\yh_\ell(x) \in \{!,?\}$: set $U = U \cup \{x\}$
\item $L(x) = L(x) + 1$
\end{itemize}
\end{itemize}

\end{minipage}}
\caption{One round of active learning.}
\label{fig:one-round}
\end{figure}

\subsection{Accuracy of bias estimates}

Our algorithm will rely upon a uniform bound on the bias estimates of all $B \in \B$, under Poisson sampling of the data. 

Recall that the pool of data points $X$ is sampled from an underlying distribution $\mu$, and that each such point $x$ is labeled according to the conditional probability $\pr(Y = 1|X = x) = \eta(x)$. Let $\eta(B)$ be the average $\eta$-value in $B$, that is,
$$ \eta(B) = \frac{1}{\mu(B)} \int_B \eta(x) \ d\mu .$$
The specific property we need is encapsulated in the following definition.
\begin{defn}
For any region $B \in \B$ and $\Delta > 0$, we say bias estimate $\yh(B) \in \{+,-,?\}$ is {\it $\Delta$-accurate} if the following holds:
\begin{itemize}
\item If $\eta(B) \geq 1/2 + 2 \Delta$, then $\yh(B) = +$.
\item If $\yh(B) = +$ then $\eta(B) > 1/2$.
\item If $\eta(B) \leq 1/2 - 2 \Delta$, then $\yh(B) = -$.
\item If $\yh(B) = -$ then $\eta(B) < 1/2$.
\end{itemize}
\label{def:accurate-bias-estimate}
\end{defn}

For any region $B \in \B$, let $X_B$ be the points in $X$ that are random draws from $\mu|_B$. The number of such points, $n(B) = |X_B|$, determines the ``level'' in which $B$ lies. We now make this precise.

In what follows, take $0 < \delta < 1$ to be a predefined constant, and define
\begin{equation}
c_1 = \sqrt{2 \ln \frac{2|\B|}{\delta}} .
\label{eq:c1}
\end{equation}

\begin{defn}
For any region $B \in \B$, set 
\begin{equation}
\tau(B) = \frac{k + c_1 \sqrt{2k}}{n(B)} .
\label{eq:ball-threshold}
\end{equation}
We define the {\it query-set} for $B$ to be 
$$ \{x \in X_B: T_x \leq \tau(B)\}.$$
\label{def:query-set}
\end{defn}

Our main deviation bound is the following, a consequence of Lemma~\ref{lemma:large-deviation-single-ball} in the appendix.
\begin{thm}
With probability at least $1-\delta$, the following is true for all $B \in \B$:
\begin{enumerate}
\item[(a)] The query-set for $B$ has size at least $k$.
\item[(b)] The average label on the query-set, call it $\widehat{\eta}(B)$, satisfies
$$ \left| \widehat{\eta}(B) - \eta(B) \right| \leq \frac{c_1}{2\sqrt{k}}.$$
\end{enumerate}
\label{thm:large-deviation-bounds}
\end{thm}
\begin{proof}
These follow immediately from Lemma~\ref{lemma:large-deviation-single-ball}. Note that $X_B$ consists of $n(B)$ random draws from $\mu|_B$; picking those with $T_x \leq \tau(B)$ is tantamount to picking each of these points with probability $\tau(B)$.
\end{proof}

\begin{cor}
For $c_1$ as in (\ref{eq:c1}), define
$$ \Delta = \frac{c_1}{2 \sqrt{k}} .$$
For each $B \in \B$, let $\widehat{\eta}(B)$ be the average label on the query-set, as above, and define the bias-estimate $\yh(B)$ as follows:
$$ \yh(B)
= 
\left\{
\begin{array}{ll}
+ & \mbox{if $\widehat{\eta}(B) > 1/2 + \Delta$} \\
- & \mbox{if $\widehat{\eta}(B) < 1/2 - \Delta$} \\
? & \mbox{otherwise}
\end{array}
\right.
$$
With probability $\geq 1-\delta$, all bias estimates $\yh(B)$, for $B \in \B$, are $\Delta$-accurate.
\label{cor:accurate-bias-estimates}
\end{cor}
In what follows, we will assume that this $(1-\delta)$-probability event holds.

\subsection{Algorithmic guarantees}

Pick any $x \in \X$ with $\eta(x) > 1/2$. We say $x$ has {\it significant bias at level $\ell$} if the following is true:
\begin{itemize}
\item $\eta(B) > 1/2$ for all $B \in \B_\ell(x)$
\item $\eta(B) \geq 1/2 + 2\Delta$ for some $B \in \B_\ell(x)$
\end{itemize}
We say $x$ is {\it resolved at level $\ell$} if it has significant bias at level $\ell$ and moreover $\eta(B) > 1/2$ for all $B \in \B_{\ell'}(x), \ell' > \ell$.

We can make a similar definition for $x$ with $\eta(x) < 1/2$.

\begin{thm}
Suppose all bias estimates $\yh(B)$ are $\Delta$-accurate as defined above. Suppose $x$ has $\eta(x) > 1/2$ and is resolved at level $\ell$. Then:
\begin{enumerate}
\item[(a)] $\yh_\ell(x) = +$ and 
\item[(b)] for any level $\ell' > \ell$, we have $\yh_{\ell'}(x) \in \{+, ?\}$ and $x \not\in U_{\ell'}$.
\end{enumerate}
Likewise if $\eta(x) < 1/2$.
\end{thm}

\begin{proof}
Suppose $\eta(x) > 1/2$ and $x$ is resolved at level $\ell$.

For any $\ell' \geq \ell$ and $B \in \B_{\ell'}(x)$, we have $\eta(B) > 1/2$ and thus $\yh(B) \in \{+,?\}$. Moreover, since $x$ has significant bias at level $\ell$, there is some $B_o \in \B_\ell(x)$ with $\eta(B_o) > 1/2 + 2\Delta$ and thus $\yh(B_o) = +$.

These imply that $\yh_\ell(x) = +$ and $\yh_{\ell'}(x) \in \{+,?\}$ for all $\ell' > \ell$. Thus, if $x \in U_\ell$, it will be removed from $U_\ell$ as soon as it is processed at level $\ell$. It will never be added to $U$ thereafter.
\end{proof}


\pagebreak
\appendix

\section{Technicalities}

\subsection{Large deviation bounds}

We need large deviation bounds that will guarantee two properties: (i) regions of interest receive at least $k$ samples, and (ii) the estimated label bias within these regions is close to the true bias.

For both results, we have an instance space $\X$ with underlying distribution $\mu$, and label space $\Y = \{-1,+1\}$ with conditional probability distribution $\eta$.
The first bound is comes from [BBL05].
\begin{lemma}
Let $\B$ consist of finitely many subsets of $\X$. Pick any $0 < \delta < 1$. Suppose $n$ points $S$ are drawn i.i.d.\ from a distribution $\mu$ on $\X$. Then with probability at least $1-\delta$ over the random sample, for all $B \in \B$, we have
$$ \mu(B) \geq \frac{k}{n} + \frac{5 \sqrt{k \ln (4|\B|/\delta)}}{n} \ \implies \ |S \cap B| \geq k,$$
provided $k \geq \ln (4|\B|/\delta)$.
\label{lemma:points-in-balls-finite}
\end{lemma}
\begin{proof}
From [BBL05, Thm 5.1], we have that with probability $> 1-\delta$, for all $B \in \B$, 
$$ \frac{|S \cap B|}{n} \geq \mu(B) - 2 \sqrt{\frac{\mu(B) \ln (4|\B|/\delta)}{n}} .$$ 
If $k \geq \ln (4|\B|/\delta)$ and $\mu(B)$ is written in the form
$$ \mu(B) = \frac{k + \alpha\sqrt{k \ln (4|\B|/\delta)}}{n} $$
for some $\alpha > 0$, then by algebraic manipulation we get $|S \cap B| \geq k$ as long as $\alpha \geq 5$.
\end{proof}

If $\B$ is a class of subsets of $\X$ of VC dimension $d_o$, then the same bound holds with $|\B|$ replaced by $d_o \ln n$.

\iffalse
Here is a weaker bound that is a bit cleaner.
\begin{lemma}[CD10, Lemma 7]
There is a universal constant $c_1$ such that the following holds. Let $\B$ be a class of measurable subsets of $\X$ of VC dimension $d_o$. Pick any $0 < \delta < 1$. Then with probability at least $1-\delta$, if a set of $n$ points $S$ is drawn i.i.d.\ from $\mu$, then for any integer $k$,
$$ \mu(B) \geq \frac{k}{n} + \frac{c_1}{n} \max \left( k, d_o \log \frac{n}{\delta} \right)
\ \implies \ 
|S \cap B| \geq k.$$ 
\label{lemma:points-in-balls-vc}
\end{lemma}
\fi

The next lemma, bounding empirical biases, is from [BDFM19].
\begin{lemma}[BDFM19, Lemma 7]
There is a universal constant $c_2$ for which the following holds. Let $\mathcal{C}$ be a class of subsets of $\X$ with VC dimension $d_o$. Suppose $n$ points $(x_1, y_1), \ldots, (x_n, y_n)$ are drawn i.i.d.\ from the underlying distribution on $\X \times \Y$ given by $\mu$ and $\eta$, and are used to determine empirical biases
$$ \eta_n(C) = \frac{|\{i \in [n]: x_i \in C, y_i = 1\}|}{n}.$$
Pick any $0 < \delta < 1$. Then with probability at least $1-\delta$ over the sample, for all $C \in \mathcal{C}$,
$$ \left| \eta_n(C) - \eta(C) \right| \ \leq \ \Delta(n, \#_n(C), \delta) $$
where $\#_n(C) = |\{i: x_i \in C\}|$ is the number of points in $C$ and 
\begin{equation}
\Delta(n,k,\delta) = c_2 \sqrt{\frac{d_o \log n + \log (1/\delta)}{k}} .
\label{eq:delta-defn}
\end{equation}
\label{lemma:bias}
\end{lemma}

\subsection{Smoothness condition}

The work of [CD14] introduced a nearest neighbor-inspired variant of the Holder condition on $\eta$: for some constants $\alpha, L > 0$, for all $x \in \supp(\mu)$ and all $r > 0$,
$$ |\eta(B(x,r)) - \eta(x)| \leq L \mu(B^o(x,r))^\alpha .$$
It was shown in [CD14, Lemma 18] that under this condition, for any $p, \gamma$,
$$ \partial_{p,\gamma} \cap \supp(\mu) \subset M(\gamma + Lp^\alpha) ,$$
where $M(\tau)$ is a shorthand for $\{x \in \X: |\eta(x) - 1/2| \leq \tau\}$.

In this paper, we consider a stronger condition.
\begin{defn}
We say $\eta$ is $(\alpha, L)$-smooth if for all $x \in \supp(\mu)$ and all $p > 0$,
$$ |\eta(x') - \eta(x)| \leq L p^\alpha \ \ \mbox{for all $x' \in B(x,r_p(x))$}.$$
\end{defn}

\begin{lemma}
Suppose $\eta$ is $(\alpha, L)$-smooth. Then for any $p,q > 0$,
$$ \bigcup_{x \in \partial_{p, \gamma}} B(x, r_q(x)) \ \subset \ M(\gamma + L(p^\alpha + q^\alpha)).$$
\label{lemma:boundary-plus}
\end{lemma}
\begin{proof}
Pick any $x \in \partial_{p, \gamma}$ and $x' \in B(x,r_q(x))$. Then
$$
|\eta(x') - 1/2| 
\ \leq \ 
|\eta(x) - 1/2| + |\eta(x') - \eta(x)|
\ \leq \ 
(\gamma + Lp^\alpha) + Lq^\alpha,
$$
using the smoothness of $\eta$ along with Lemma 18 of [CD14].
\end{proof}

A more common assumption is that $\X = \R^d$ and $\eta$ is $\alpha_H$-Holder continuous for some $\alpha_H > 0$, that is,
$$ |\eta(x) - \eta(x')| \leq C \|x-x'\|^{\alpha_H}$$
for some $C > 0$. Typically such analysis also requires $\mu$ to be bounded below. Here we relate this to our assumption.
\begin{lemma}
Suppose that $\X \subseteq \R^d$, that $\eta$ is $\alpha_H$-Holder continuous, and that for some $\mu_{\scriptsize\rm min} > 0$ it is the case that 
$$\mu(B(x,r)) \geq \min(\mu_{\scriptsize\rm min} r^d,1)$$ 
for all $x \in \X$ and $r > 0$. Then there is a constant $L > 0$ such that $\eta$ is $(\alpha_H/d, L)$-smooth.
\end{lemma}
\begin{proof}
Suppose $\eta$ satisfies the $\alpha_H$-Holder condition: for some $C > 0$,
$$ |\eta(x) - \eta(x')| \leq C \|x-x'\|^{\alpha_H}.$$
Now pick any $x \in \X$, any $p > 0$, and a point $x' \in B(x, r_p(x))$. Then
$$ |\eta(x) - \eta(x')| 
\leq C \|x-x'\|^{\alpha_H}
\leq C \left( \frac{\mu(B(x, \|x-x'\|))}{\mu_{\scriptsize\rm min}} \right)^{\alpha_H/d}
\leq C \left( \frac{p}{\mu_{\scriptsize\rm min}} \right)^{\alpha_H/d}.
$$
The result follows by taking $L = C \mu_{\scriptsize\rm min}^{-\alpha_H/d}$.
\end{proof}



\end{document}
