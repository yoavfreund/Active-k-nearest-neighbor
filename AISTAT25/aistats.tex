\documentclass[twoside]{article}

\usepackage{aistats2024}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{times}
\usepackage{amssymb}
\usepackage{graphicx}


\def\R{{\mathbb{R}}}
\def\pr{{\rm Pr}}
\def\E{{\mathbb E}}
\def\X{{\mathcal X}}
\def\Y{{\mathcal Y}}
\def\H{{\mathcal H}}
\def\G{{\mathcal G}}
\def\B{{\mathcal B}}
\def\yh{{\widehat{y}}}
\def\bias{{\rm bias}}
\def\supp{{\rm supp}}
\def\dist{{\rm dist}}
\def\sign{{\rm sign}}
\def\vol{{\rm vol}}
\def\PL{{\mbox{\rm PL}}}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
%\newtheorem{claim}[thm]{Claim}
\newtheorem{defn}[thm]{Definition}
\newtheorem{assump}{Assumption}
\newenvironment{proof}{\noindent {\sc Proof:}}{$\Box$ \medskip}

\newcommand{\comment}[2]{ {\bf #1 :} #2}
\newcommand{\yoav}[1]{\comment{Yoav}{\em #1}}
\newcommand{\sanjoy}[1]{\comment{blue}{Sanjoy}{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}

%\setlength{\leftmargini}{0.5cm}
%\setlength{\leftmarginii}{0.25cm}
%\setlength{\leftmarginiii}{0.25cm}


% If your paper is accepted, change the options for the package
% aistats2024 as follows:
%
%\usepackage[accepted]{aistats2024}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Nonparametric active learning without smoothness}

\aistatsauthor{ Sanjoy Dasgupta \And Yoav Freund}

\aistatsaddress{University of California, San Diego} ]


\begin{abstract}%
  We present a general-purpose active learning scheme that maintains a
  collection of balls of different sizes and uses label
  queries to identify those with a strong bias towards one particular
  label. When two such balls intersect and have different
  labels, the region of overlap is treated as a ``known unknown'' and
  is targeted in future active queries. We give label complexity
  bounds for this method that do not rely on assumptions about the
  data, and we instantiate them in several cases of interest.
\end{abstract}

\section{Introduction}

In \emph{non-parametric active learning}, the starting point is a data
set whose labels are hidden but can be obtained individually for a
price. The goal is to label the data set, or to find a good
classifier, at low cost.

We consider a formulation in which we have an arbitrary collection of
points $X = \{x_1, \ldots, x_n\}$ from some metric space
$\X$.~\footnote{Our approach works for any topological space
  that defines a sufficiently rich set of neighborhoods. For the same of concreteness, we
  restrict ourselves to metric spaces where the neighborhoods are balls.}  We can
query the label of any of these points $x$, in which case we get a
value $y \in \{-1, +1\}$ with conditional expectation
$ \eta(x) = \E[y | x] = \pr(y=1|x) - \pr(y=-1|x) $ for some arbitrary
unknown function $\eta: \X \to [-1,1]$. The \emph{Bayes-optimal} label
for $x$, which we shall denote $g^*(x)$, is $-1$ if $\eta(x) < 0$ and
$+1$ if $\eta(x) > 0$; either label can be used if $\eta(x) = 0$.  We
wish to find an algorithm that approaches the Bayes-optimal labels for
the points in $X$ using only a small number of queries, called the
{\em query complexity} of the algorithm.

More precisely, at the outset we have the data $X$ and parameters
$0 < \gamma, \delta < 1$. We want a procedure that chooses the next
point, or batch of points, to query. This procedure will be applied
repeatedly and can be stopped at any time, whereupon labels $\yh(x)$
must be provided for all $x \in X$, including those that were
queried. Ideally, these will be the Bayes-optimal labels $g^*(x)$;
however, we will only be judged on points with non-negligible bias,
$|\eta(x)| \geq \gamma$. The number of mistakes is:
$$ \sum_{x \in X} {\bf 1} \big(\yh(x) \neq g^*(x),\ |\eta(x)| \geq \gamma \big) .$$
The overall procedure is allowed to fail with probability $\delta$, to account for sampling error.


Prior work on this problem~(see Section~\ref{sec:relatedwork})
requires making various assumptions, typically on the smoothness of
$\eta(x)$.  In contrast, we make no assumptions on either the set $X$
or on the conditional expectation $\eta(x)$. Our algorithm is
universal and works for any $X$ and $\eta(x)$. The {\em query
  complexity} of our algorithm depends on $X$ and $\eta$, but it does
not depend on smoothness of $\eta(x)$. Instead it depends on
properties of $\eta(x)$ close to the decision boundary where $\eta(x)$
changes sign. In particular, the algorithm is effective when $\eta(x)$
is discontinuous and hence not smooth.

\subsection{Approach}

We begin with a large collection of \emph{balls}, $\B$, of varying
sizes (Fig.~\ref{fig:challenges}(a)). We use label queries to assign
\emph{biases} $\yh(B) \in \{-1,0,+1\}$ to balls $B \in \B$, beginning
with the largest balls. A bias of $+1$ means that the average of 
$\eta(x)$ over $s \in B$ is significantly positive; bias $-1$ means it is
significantly negative; and bias $0$ means the average $\eta$ is close
to 0. As time goes on, we get biases for progressively smaller balls,
as needed. The final label assigned to any point $x$ is based on the
biases of balls containing just $x$.

\begin{figure}
\begin{center}
\includegraphics[width=3in]{figures/ideas.pdf}
\end{center}
\caption{(a) A collection of balls $\B$. Ball $B$ has bias $\yh(B) \in \{-1,0,+1\}$. (b) If one ball has bias $+1$ and the other $-1$, points in their intersection are targeted for active querying. (c) Biases of progressively smaller balls containing $x$ might flip sign, leading to \emph{mind changes} about the label of $x$.}
\label{fig:challenges}
\end{figure}

There are three key challenges to be addressed. 

\begin{enumerate}
\item \label{challange:wheretoquery}
  {\bf Where to query?} \\
  Initial random querying tells us the biases of the largest balls,
  which gives some information about the decision boundary. In what
  parts of $\X$ might it be helpful to know biases of smaller balls,
  warranting further label queries? Our approach is simple: if a ball
  $B$ of bias $+1$ intersects a ball $B'$ of bias $-1$ then the
  intersection $B \cap B'$ is a ``known unknown'' and is targeted for
  active querying (Fig.~\ref{fig:challenges}(b)).

  This differs from the more conventional strategy of actively
  querying balls $B$ whose average $\eta$-value is close to zero. Such
  $B$ fall into two categories: (1) the $\eta$ values are close to
  zero throughout $B$, and (2) $B$ consists of two sub-regions, one
  strongly positive and the other strongly negative. In the first
  case, there is little merit in querying further. But in the second
  case, there is a lot to be gained. Our approach allows these two
  cases to be distinguished.

%To distinguish these two cases, we use a collection of balls that are \emph{overlapping}. For instance, we might take $\B$ to be \emph{all} balls in $\X$, which is effectively a finite collection once the given data points $X$ are taken into account. Case (2) can then be detected: we think of a point $x$ as being in the uncertainty region if it is contained in a ball $B$ that is strongly positive as well as being in a ball $B'$ that is strongly negative. Such points are ``known unknowns'', and these are the targets of our active querying.

\item \label{challange:nosmoothness}
  {\bf No smootheness assumptions}\\
  As no assumptions of smoothness are made, the sign of $\eta(\cdot)$
  can change in an arbitrarily small ball of any given point: the
  biases of balls containing $x$, say
  $B_1 \supset B_2 \supset \cdots$, might change sign a few times. At
  first, given only the bias of $B_1$, we might think $x$ has label
  $+1$. When we later estimate the bias of $B_2$, we could change our
  mind to $-1$. And then $+1$, and so on
  (Fig.~\ref{fig:challenges}(c)).


\item \label{challange:managingsampling}
  {\bf Managing sampling} \\
  The third challenge is \emph{managing sampling} of overlapping
  balls. We are interested in detecting points, and thus balls, whose
  $\eta$-values are either $> \gamma$ or $<-\gamma$. This suggests
  querying $k \approx 1/\gamma^2$ points at random from each
  ball. Now, suppose we have queried this many points from ball $B$
  and later want to query points from a different ball $B'$ that
  overlaps $B$. How can we reuse queries already made in $B \cap B'$?

\end{enumerate}

These three challenges go beyond earlier work in active learning,
which was able to avoid problems like mind-changes by making
smoothness assumptions on $\eta$. By tackling all three of them, we
are able to give a universal active learning scheme. Our focus is on
the query complexity of the algorithm, largely ignoring the
computational complexity. However, we do provide some experimental results that demonstrate that the algorithm works on a toy example.

The rest of the paper is organized as
follows. Section~\ref{sec:relatedwork} describes related work.
Section~\ref{sec:algorithm} presents the active learning algorithm and
Section~\ref{sec:discrete} analyzes it in the finite population
setting, taking $X$ to be an arbitrary point-set and allowing any
$\eta$ function. We identify two \emph{critical levels} for any
$x \in X$: two scales (of ball sizes) that control how many queries
are sufficient for $x$ to be correctly labeled
(Theorem~\ref{thm:label-complexity}). We instantiate these bounds in
canonical one-dimensional settings (Theorems~\ref{thm:oned-massart}
and \ref{thm:oned-monotonic}) to get label complexities logarithmic in
$|X|$. In section~\ref{sec:experiment} we describe an experiment that supports the theoretical bound given in Theorems~\ref{thm:oned-massart}.

The supplementary material in
Section~\ref{sec:FinitePopulationProofs}, which contains the proofs of
the theorems from Section~\ref{sec:discrete}.  Finally,
Section~\ref{sec:continuous} considers the statistical setting where
$X$ is drawn from an underlying distribution $\mu$ on $\X$. We give
rates of convergence under distributional conditions
(Theorem~\ref{thm:label-complexity-specific}), and instantiate them
under common assumptions from learning theory and computational
geometry.

\subsection{Related work} \label{sec:relatedwork}

Existing work on non-parametric active learning yields algorithms that are guaranteed to work when the underlying conditional expectation $\eta(x)$ is smooth in some way. Additional assumptions are often needed.

This work can mostly be grouped by overall principle: either (1) they
seek to obtain the Bayes-optimal labels of a few well-positioned
points, and then propagate these to the rest of the space
\cite{DNZ15,H17,ASU18} or (2) they estimate the biases (positive or
negative) of entire regions at a time \cite{DH08,M12}. In this paper,
we follow the second strategy because the only reliable and
general-purpose way to assess the bias of an individual point,
$\mbox{sign}(\eta(x))$, is to query that point repeatedly; absent
smoothness assumptions, the sign can change abruptly in an arbitrarily
small ball around $x$. On the other hand, the bias of a region
$B$---the sign of the average $\eta$ value in $B$---is easy to
determine given a reasonable number of queries chosen at random from that region.

Early results of \cite{CN08} established upper and lower bounds on label complexity in situations where the Bayes-optimal boundary is of a simple form: a single threshold for one-dimensional data or a ``smooth boundary fragment'' in higher dimension.

An algorithm for active learning based on hierarchical sampling was given by \cite{DH08} and was analyzed under smoothness conditions by \cite{KUB15}. The idea is to begin with a hierarchical clustering of $X$, and to then use queries to discover a pruning of this tree whose leaf-clusters are almost-pure in their labels. The method is not well-suited to situations with significant noise levels. Another approach using dyadic partitions was given by \cite{M12} and analyzed under commonly-used smoothness, margin, and density assumptions---namely, that $\eta$ is Holder-smooth, the fraction of points with $|\eta(x)| \leq t$ is some polynomial in $t$, and the marginal density is close to uniform---along with an additional smoothness requirement on $\eta$.

A different strategy using nearest neighbors was explored by \cite{ASU18}. Their idea was to choose an appropriate scale $s$, find an $s$-cover of $X$, estimate the Bayes-optimal label for each point in this cover by querying its neighbors, and then use these cleanly-labeled points for 1-nearest neighbor classification. A somewhat more general approach was given by \cite{H17} and studied under the usual smoothness, margin, and density conditions, with resulting rates of convergence comparable to those found by \cite{M12}.

Finally, \cite{DNZ15} suggested a graph-based method for active learning based on adaptively looking for the cut in the graph corresponding to the correct decision boundary. Their assumptions are based on properties of this cut and are not easily comparable with earlier work.

The idea of predicting with balls of multiple sizes is a central theme of \cite{BDFM19}, which predicts at a point $x$ by choosing a ball centered at $x$ that is as small as possible while having a clear bias towards one label. That work does not consider active learning, which demands considerable further innovation. For instance, to assess the uncertainty at a point $x$, we need to look at balls containing $x$, not just those centered at $x$ (Fig.~\ref{fig:challenges}(b)).


Recall that the algorithm can query any $x \in X$ and receive $-1$ or $+1$ at random,
selected according to the conditional expectation function $\eta(x) = \E[y|x]$.

The goal of the algorithm is to make a small number of queries and then
assign Bayes-optimal labels to all points in $X$ with $|\eta(x)| \geq \gamma$.

The goal of our analysis is to characterize conditions under which the
number of queries is significantly smaller than
$\frac{|X|}{\gamma^2}\log \frac{1}{\gamma^2}$ which is the expected
  number of number of queries made by the naive algorithm that queries
  random elements of $X$ until each $x \in X$ has been sampled
  $\frac{1}{\gamma^2}$ times.

\section{Definitions}

The two basic entities in our algorithm are the collection of points
$X$ and a collection of balls $\B$. Sampling is organized around a the
collection $\B$. Balls are the atomic sets on which we assess label
bias.

For any ball $B \in \B$, let $X_B = X \cap B$ be a shorthand for the
data points that lie in it.  We group balls into levels by the number
of points they contain. We put $B$ at {\it level} $\ell \geq 0$ if
\begin{equation}
\frac{|X|}{2^{\ell + 1}} \leq |X_B| < \frac{|X|}{2^\ell} .
\label{eq:sampling-level}
\end{equation}
Let $\B_\ell$ consist of all balls in $\B$ that are at level
$\ell$. Thus $\B_0, \B_1, \ldots$ is a partition of $\B$, with $\B_0$
consisting of highly-populated balls and subsequent
$\B_1, \B_2, \ldots$ consisting of successively smaller balls. We will
use $\B_{\geq \ell}$ to denote all balls at levels $\ell$ or greater,
and likewise $\B_{> \ell}$, $\B_{\leq \ell}$, and so on.

Balls in lower levels contain more points, and thus their biases
(average $\eta$ values) are easier to estimate. The main loop of the
simplified learning algorithm iterates over the levels $\ell=0,1,2,\ldots$

For any $x \in \X$, let $\B(x) \subset \B$ denote the collection of
balls that contain $x$ and can thus be used in determining $x$'s
label. We again partition these balls by sampling-level, so that
$\B_\ell(x) = \B(x) \cap \B_\ell$.

\section{A general-purpose active learning algorithm}
\label{sec:algorithm}
Our algorithm has several intricacies that might make it hard to
follow at a first read.  We therefor start with a description of a
simplified version of the algorithm (Fig~\ref{alg:simple}) that is not
completely correct but serves to explain the main components of the
detailed algorithm. The detailed algorithm is described in
subsection~\ref{sec:detailedalgorithm}.

\begin{figure*}[t]{6in}
\framebox{
\begin{minipage}[t]{6in}
\begin{center}
\vspace{.1in}
\textbf{Repeat for $\ell=0,1,2,\ldots,\log |X|-1$:}
\begin{enumerate}[leftmargin=0.6cm]
\item \label{step:Uell} {\bf Compute uncertainty region:}\\ If $\ell=0$, set $U_0 = X$; otherwise set $U_\ell = \{x \in X: \yh_{\ell-1}(x) = \,!, \ \yh_\ell(x) = \bot\}$t
\item \label{step:QS} {\bf Compute Focused Query region:}\\ Compute query region be $R=  \bigcup_{x \in U} \B_\ell(x)$
and focused query region $S=\bigcup_{B \in R} B$
\item \label{step:sample} {\bf Make queries:}\\
\textbf{Repeat until} number of queries in each ball $B \in R$ is at least $C/\gamma^2$
  \begin{enumerate}[leftmargin=0.6cm]
  \item Select a {\tt query} uniformly at random from $S$
  \end{enumerate}

\item \label{step:balls} {\bf Update Ball labels:} For each ball $B \in R$, let $k(B)$ be
  the number of queries (of any kind) made inside $B$ and let
  $\widehat{\eta}(B)$ be the average of the labels returned by
  these queries.

  $$ \yh(B)
  =
  \left\{
    \begin{array}{ll}
%%%      \bot & \mbox{if $k(B) \leq \frac{C}{\gamma^2}$} \\
%%%      \sign(\widehat{\eta}(B)) & \mbox{if  $k(B) > \frac{C}{\gamma^2}$ and $|\widehat{\eta}(B)| \geq \gamma/2$} \\
      \sign(\widehat{\eta}(B)) & \mbox{if  $|\widehat{\eta}(B)| \geq \gamma/2$} \\
      0 & \mbox{otherwise}
    \end{array}
  \right.
  $$

\item \label{step:examples} {\bf Update example labels:} For each $x \in U_\ell$:
  \begin{align}
    \PL_\ell(x) &= \{s \in \{-1, +1\}: \\
               & \text{s.t. there exists } B \in B_\ell(x) \text{ s.t. } \yh(B) = s \} \\ 
    \yh_\ell(x) &= 
                  \begin{cases}
                    +1 & \text{if } \PL_\ell(x) = \{+1\} \\
                    -1 & \text{if } \PL_\ell(x) = \{-1\} \\
                    0  & \text{if } \PL_\ell(x) = \{\} \\
                    !  & \text{if } \PL_\ell(x) = \{-1,+1\}
                  \end{cases}
                  \label{eq:provisional-label}
  \end{align}
\end{enumerate}
\end{center}

\end{minipage}}
\caption{A simplified version of the active learning algorithm.}
\label{alg:simple}
\end{figure*}

\subsection{The simplified algorithm}

We now describe the simplified algorithm here referred to simply as
the algorithm. The algorithm loops over increasing levels.  At each
level it identifies the region of ``known unknowns'' which approximates
the boundary. The algorithm then makes queries in the vicinity of the
boundary to refine it and proceeds to the next finer level.

What follows is a detailed explanation of  Figure~\ref{alg:simple}.
The outer loop of the algorithm iterates over the
levels, starting with $\ell=0$ that corresponds to balls that contain
at least half of the points in $X$, and ending with $\ell=\log |X|-1$,
corresponding to balls that contain a single point. Initially, all balls are labeled $\bot$, which indicates that there are not enough queries inside the ball to determine it's label.

The loop consists of five steps. The first computes the {\em uncertainty
region}, the second computes the {\em focused query region}, the third defines the
sampling process, the fourth and fifth update the labels of the balls
and the points respectively.

In step~\ref{step:Uell} the {\em uncertainty region} $U_\ell$ is
calculated. Intuitively $U_l$ is the set of points whose label
according to level $\ell-1$ is ! or ``known unknown'' and the label
according to level $\ell$ is $\bot$ or ``not enough information''.
These are the points for which we need better estimates.

In step~\ref{step:QS} the algorithm identifies the balls that needs to
be queried in order to get more information about $U_\ell$. These are
the balls in $\B_l$ that contain at least one point of $U_\ell$.  The
region $S$ is the union of these balls and defines the set
from which queries are selected in order to reduce the known unknowns.

In step~\ref{step:sample} the algorithm queries points inside the
focus region repeatedly at random.  This step terminates once there are
$C/\gamma^2$ samples in each of the balls in the focus region.

Step ~\ref{step:balls} associates a label with each ball in the focus region. The
label is one of $\{-1,+1,0\}$.  The labels $+1$ and $-1$ correspond to
a confident statement that $\eta(B)>0$ and $\eta(B)<0$
respectively. Finally, the label $0$ indicates that the sign of $B$
is uncertain given the answers to the queries.

In step~\ref{step:examples} these labels are propagated to the
examples.  This is done in two sub-steps. First the {\em Possible
  Labels} for $x$ denoted $\PL_\ell(x)$ are calculated. $\PL_\ell(x)$
contains $+1$ if there is a ball containing $x$ whose label is $+1$,
symmetrically for $-1$. The label $\yh_\ell(x)$ is determined by the
possible labels set $\PL_\ell(x)$ as described in
Equation~\ref{eq:provisional-label}. The most significant label is ``!'', which
indicates balls with conflicting polarity containing $x$. Points
labeled ``!'' determine the uncertainty region and the focused query
region in the following level and the process repeats.

\iffalse
  \subsection{Estimating the biases of balls}

The bias of a ball $B \in \B$ is defined as the average $\eta$ value in it,
$$ \eta_X(B) = \mbox{average}\{\eta(x): x \in X_B \} .$$
We estimate these biases using label queries and assign each ball a
\emph{qualitative} bias estimate $\yh(B)$,

Suppose that $k'$ queries have been made in the ball $B$ and let
$\widehat{\eta}(B)$ denote the mean of the answers to these queries.

step \ref{step:Uell} and \ref{step:QS} and \ref{step:balls} and \ref{step:sample} and \ref{step:examples}

We set the qualitative bias estimate as follows:
$$ \yh(B)
= 
\left\{
  \begin{array}{ll}
    \bot & \mbox{if $k \leq \frac{1}{\gamma^2}$} \\
\sign(\widehat{\eta}(B)) & \mbox{if  $k > \frac{1}{\gamma^2}$ and $|\widehat{\eta}(B)| \geq \gamma/2$} \\
0 & \mbox{otherwise}
\end{array}
\right.
$$

The option $\bot$ is used until enough points in $X_B$ have been queried: the required number is $k = O(1/\gamma^2)$ since $\gamma$ is the smallest bias that needs to be detected. Once this many labels are available, $\yh(B)$ is set to a value in $\{-1,0,+1\}$ and remains fixed thereafter.

These bias estimates will with high probability be seen to satisfy the following guarantee.
\subsection{Assigning labels to points}

The label of any point is inferred from the biases of balls containing it. 

Pick any $x \in X$ and any level $\ell$. Once qualitative bias estimates $\yh(B)$ are available for all balls at levels $\leq \ell$ that contain $x$ (that is, $\B_{\leq \ell}(x)$), we use these to infer a level-$\ell$ label for $x$. To begin with, the set of \emph{possible} labels for $x$ at level $\ell$, denoted $\PL_\ell(x) \subset \{-1,+1\}$, is defined thus:
\begin{itemize}[leftmargin=0.5cm]
\item $\PL_\ell(x)$ includes $+1$ if there exists a \emph{minimal} ball $B \in \B_{\leq \ell}(x)$ (with no other $B' \in \B_{\leq \ell}(x)$ strictly contained in it) for which $\yh(B) = +1$.
\item $\PL_\ell(x)$ includes $-1$ under a symmetrical condition.
\end{itemize}
This is made precise in Eqn~(\ref{eq:PL}) in Fig.~\ref{alg:main}. The label-estimate for $x$ at level $\ell$, denoted $\yh_\ell(x)$, is $+1$ if $\PL_\ell(x)=\{+1\}$, $-1$ if $\PL_\ell(x) = \{-1\}$, $0$ if $\PL_\ell(x) = \{\}$, and $!$ if $\PL_\ell(x) = \{-1,+1\}$ (Eqn~(\ref{eq:provisional-label})). 
The interpretation of $\yh_\ell(x) \in \{+1,-1,0\}$ is given in Definition~\ref{def:accurate-bias-estimate}. 

The label $\yh_\ell(x) = \, !$ indicates that at level $\ell$, there is conflicting evidence about $x$; it is a ``known unknown''~\cite{R11}. Our active learning algorithm makes its \emph{focused queries} at level $\ell+1$ in the vicinity of such points $x$, that is, in $\B_{\ell+1}(x)$.

% to be $-1$ if $\PL_\ell(x) = %\{-1\}$, or $+1$ if $\PL_\ell(x) = \{+1\}$, or $0$ if $\PL_\ell(x) = \{\}$, or $!$ %if $\PL_\ell(x) = \{-1,+1\}$, %and remains fixed thereafter. Points labeled $!$ are ``known unknowns'': they are %near the decision boundary and %need further investigation. Points labeled $0$ show no significant bias.

\fi




\subsection{The detailed algorithm}
\label{sec:detailedalgorithm}

The simplified algorithm does addresses the first challange of ``where
to query'' (see challanges in the introduction). It does not address
the other two challanges, which the detailed algorithm does address.

\begin{enumerate}
\item {\bf No smoothness assumptions:}\\
  As described in the introduction, one can \emph{never} be sure of having
  correctly determined the label of $x$. Thus in addition to focused
  (active) querying, we also do background sampling of the entire
  space to pick up on possible mind changes. For simplicity, we make
  one background query per focused query.

  A related problem is that having the outer loop iterate over levels
  will cause the algorithm to miss some uncertainty region if they are
  appear at a lower level. Therefor the full algorithm does not loop
  over levels. Instead, at each iteration it finds the lowest level
  where the uncertainty region is not empty and makes two queries, one
  focused and one background.

\item {\bf Managing Sampling:}\\
  As described in the introduction, we would like to use queries that
  fall in the intersection of two balls for estimating the bias in
  both balls. However, selecting examples IID at random from the balls
  can create an undesired dependence between the estimates. This is solved using Poisson sampling, as described in section~\ref{sec:poisson}.
\end{enumerate}

We now describe the detailed active learning algorithm as shown in
Figs~\ref{alg:main}, \ref{alg:sampling} and~\ref{alg:bias-estimate}.
The algorithm makes two types of query. \emph{Background queries} are random draws
from $X$ and correspond to passive learning. \emph{Focused queries}
are made in the vicinity of ``uncertain'' points and correspond to
active learning.

Many of the elements of the detailed algorithm have been introduced in the simplified version.
Specifically $U_\ell,S,\PL_\ell(x),\yh_\ell(x)$, are as defined there.

However, in the detailed algorithm all of the levels are considered at each iteration.
On each iteration of the main loop (at most) one focused
query is made as well as a background query. The focused query comes
from the lowest-numbered uncertainty region $U_\ell$ that is nonempty.

Once these queries are made, bias estimates $\widehat{\eta}(B)$ are
updated using the Poisson method,\\
$\yh(B), \PL_\ell(x), \yh_\ell(x), U_\ell$ are updated
accordingly  and the next iteration begins.

The querying process can be stopped at any time, whereupon labels are assigned as follows:
\begin{equation}
\yh(x) = 
\left\{
\begin{array}{cl}
  \yh_\ell(x) & \mbox{largest $\ell$ with $\yh_\ell(x) \in \{-1,+1\}$}\\
  & \mbox{if such an $\ell$ exists} \\
0 & \mbox{``don't know'', otherwise}
\end{array}
\right.
\label{eq:final-label}
\end{equation}

\subsubsection{Poisson Sampling}
\label{sec:poisson}


The uncertainty region at level $\ell$ consists of the known-unknowns
from level $\ell-1$, that is, points $x \in X$ with
$\yh_{\ell-1}(x) = \, !$. This region is revealed to us piecemeal, a
few points at a time. To get good label complexity we need to actively
query the part of it we know.

We manage this through \emph{Poisson
  sampling}~\cite{ghosh2002sampling} (Fig.~\ref{alg:sampling}) which
works as follows. Each instance $x \in X$ is assocciated with a random
draw from the uniform distribution over $[0,1]$ denoted $T_x$.
Pooints are sampled in the order of increasing $T_x$ under some
conditions described below.  Note that restricting the sampled points
to be from set $A$ and having $T_x \leq \tau$ is the same, in
expectation to sampling each point in $A$ independently at random with
probability $\tau$.

To infer the bias of a ball $B$, we need labels for
$k = \tilde{O}(1/\gamma^2)$ random points in it. Balls at level $\ell$
have roughly $n/2^{\ell+1}$ points, so we need about $2^{\ell+1} k/n$
fraction of the ball to be queried. Instead, we query every point in
the ball with probability $\tau_\ell = \min(2^{\ell+2}k/n, 1)$,
\emph{independently}. Specifically, we query points $x$ in the ball
with $T_x \leq \tau_\ell$, where each $x \in X$ is assigned a
uniform-random value $T_x \in [0,1]$ at the outset of the algorithm.

Once a point is queried, it is never queried again.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h]
\framebox{
\begin{minipage}[t]{3in}

Initialize uncertainty regions at all levels: $U_0 = X$ and $U_\ell = \emptyset$ for $\ell \geq 1$

%\vspace{.1in}

Initialize labels at all levels to ``unavailable'': \ $\yh_\ell(x) = \bot$ for all $x$ and $\ell \geq 0$

%\vspace{.1in}

Repeat:
\begin{itemize}[leftmargin=0.5cm]
\item If there is a level $\ell \geq 0$ such that $U_\ell \neq \emptyset$:
\begin{itemize}[leftmargin=0.25cm]
\item For the smallest such level $\ell'$, run {\tt Focused-query}($\ell', U_{\ell'}$) // Fig.~\ref{alg:sampling}
\end{itemize}
\item Run {\tt Background-query} // Fig.~\ref{alg:sampling}
\item Update labels:
\begin{itemize}[leftmargin=0.25cm]
\item Update bias-estimates $\yh(B)$ // Fig.~\ref{alg:bias-estimate}
\item For each $x \in X$ and level $\ell \geq 0$ for which all $\{\yh(B): B \in \B_\ell(x)\}$ are available:
\begin{align}
  \PL_\ell(x) &= \{s \in \{-1, +1\}: \\
             & \mbox{there exists $B \in \B_{\leq \ell}(x)$ s.t. $\yh(B) = s$ and} \notag \\ 
             & \mbox{no $B' \in \B_{\leq \ell}(x)$ has $X_{B'} \subsetneq X_B$.} \} \label{eq:PL} \\ 
\yh_\ell(x) &= 
\left\{
\begin{array}{cl}
+1 & \mbox{if $\PL_\ell(x) = \{+1\}$} \\
-1 &  \mbox{if $\PL_\ell(x) = \{-1\}$} \\
0 & \mbox{if $\PL_\ell(x) = \{\}$} \\
! & \mbox{if $\PL_\ell(x) = \{-1,+1\}$}
\end{array}
\right.
\label{eq:provisional-label1}
\end{align}

\end{itemize}
\item Update uncertainty regions:
\begin{itemize}[leftmargin=0.25cm]
\item $U_0 = \{x \in X: \yh_0(x) = \bot\}$
\item For all levels $\ell \geq 1$: \ $U_\ell = \{x \in X: \yh_{\ell-1}(x) = \,!, \ \yh_\ell(x) = \bot\}$
\end{itemize}
\end{itemize}

\end{minipage}
%\end{center}
%\end{minipage}
}
\caption{The active learning algorithm. Each iteration of the main loop makes (at most) one focused query and one background query.}
\label{alg:main}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
\framebox{
\begin{minipage}[t]{3in}

\vspace{.05in}
\emph{Initialization:}
\begin{itemize}[leftmargin=0.5cm]
\item Set $Q = \emptyset$ (points queried so far)
\item For each $x \in X$: choose $T_x \sim \mbox{uniform}([0,1])$
\end{itemize}

\vspace{.05in}
{\bf Background-query}

\begin{itemize}[leftmargin=0.5cm]
\item Query the next unlabeled point in $X \setminus Q$, ordered by increasing $T_x$ values, and add to $Q$
\end{itemize}

\vspace{.05in}
{\bf Focused-query}($\ell, U$)

\begin{itemize}[leftmargin=0.5cm]
\item Define focus region:
$$ S =  \bigcup_{x \in U} \bigcup_{B \in \B_\ell(x)} \{z \in X_B: T_z \leq \tau_\ell\} $$
\item Query the next unlabeled point in $S \setminus Q$, ordered by increasing $T_x$ values, and add to $Q$
\end{itemize}

\end{minipage}}
\caption{The two sampling procedures. Each $x \in X$ has an associated r.v. $T_x$ drawn uniformly from $[0,1]$. The smaller $T_x$, the earlier $x$ is likely to be queried. \emph{Background queries} are drawn at random from $X$. \emph{Focused queries} are from the uncertainty region at a given level $\ell$, and use level-based thresholds $\tau_\ell = \min(2^{\ell+2}k/n, 1)$, where $k = \tilde{O}(1/\gamma^2)$.} 
\label{alg:sampling}
\end{figure}


\begin{figure}
\framebox{
\begin{minipage}[t]{3in}
\vspace{.05in}
\begin{itemize}[leftmargin=0.25cm]
\item Initially $\yh(B) = \bot$
\item When all of $\{z \in X_B: T_z \leq \tau_\ell\}$ is queried, let $\widehat{\eta}(B)$ be the mean of these labels and set
$$ \yh(B)
= 
\left\{
\begin{array}{ll}
\sign(\widehat{\eta}(B)) & \mbox{if $|\widehat{\eta}(B)| \geq \gamma/2$} \\
0 & \mbox{otherwise}
\end{array}
\right.
$$
\end{itemize}
\end{minipage}}
\caption{The qualitative bias $\yh(B)$ of a ball $B \in \B_\ell$; see Fig.~\ref{alg:sampling} for $\tau_\ell$.}
\label{alg:bias-estimate}
\end{figure}

\section{Analysis: finite population setting}
\label{sec:discrete}

We now analyze the active learning procedure in a setting where $X \subset \X$ is an arbitrary set of $n$ points. We make no distributional assumption on the manner in which $X$ is generated.

\subsection{Accuracy of bias estimates}

Fix the set of balls $\B$ and let $0 < \delta < 1$ be a
predefined confidence parameter. We start with a uniform guarantee on the
bias estimates for all balls $B \in \B$.

In Fig.~\ref{alg:sampling}, we see that the points queried in a ball $B$ at level $\ell$ are $\{z \in X_B: T_z \leq \tau_\ell\}$, which from the definition of $\tau_\ell$ has size $O(k)$. We use these labels to estimate the bias of $B$. Since we need to detect biases of magnitude greater than $\gamma$, we set $k \propto 1/\gamma^2$. 
%This intuition is borne out by the following result, proved in the appendix. 

In the next definition and lemma we establish that the bias estimates for all balls in $\B$ are sufficiently accurate. 
\begin{defn}
\label{def:accurate-bias-estimate}
For any $B \in \B$, bias estimate $\yh(B) \in \{+1,-1,0\}$ is \emph{$\gamma$-accurate} if:
\begin{itemize}
\item $\yh(B) = +1 \implies \eta_X(B) > 0$
\item $\yh(B) = -1 \implies \eta_X(B) < 0$
\item $\yh(B) = 0 \implies |\eta_X(B)| < \gamma$
\end{itemize}
\end{defn}


\begin{lemma}
\label{lemma:accurate-bias-estimate}
  Suppose that $k \geq (192/\gamma^2) \ln (4 |\B|/\delta)$.  Let
  $\yh(B)$ be defined as in Fig.~\ref{alg:bias-estimate}.  Then with
  probability $\geq 1-\delta$, all the $\yh(B)$, for $B \in \B$, are
  $\gamma$-accurate in the sense of
  Definition~\ref{def:accurate-bias-estimate}.
\end{lemma}
Henceforth assume $\yh(B)$ is $\gamma$-accurate for all  $B \in \B$.


\subsection{Critical levels}

Recall that $\PL_\ell(x) \subset \{-1,+1\}$ denotes the possible labels for $x$ given information from balls at levels $\leq \ell$, and is used to assign a value $\yh_\ell(x) \in \{-1,+1,0,!\}$. Despite randomness in querying, it is possible to define two critical levels for each $x$: a level $L_1(x)$ by which $\PL_\ell(x)$ reliably contains the correct label of $x$, and a level $L_2(x)$ by which $\PL_\ell(x)$ reliably omits the wrong label. Intuitively, background sampling dominates until level $L_1(x)$ is eached while focused sampling dominates between $L_1(x)$ and convergence at $L_2(x)$.
Therefor, roughly speaking, the query complexity for point $x$ is low when $L_2(x)-L_1(x)$ is large relative to $L_1(x)$. The precise statement is given in Theorem~\ref{thm:queryComplexity}.

\begin{defn}[Critical levels $L_1,L_2$]
Pick any $x \in X$ with $\eta(x) \neq 0$ and let $s(x) = \sign(\eta(x))$. Define $L_1(x)$ to be the smallest level $\ell$ such that there exists $B_o \in \B_\ell(x)$ for which
\begin{itemize}[leftmargin=0.5cm]
\item $s(x) \cdot \eta_X(B_o) \geq \gamma$, and
\item $s(x) \cdot \eta_X(B) \geq \gamma$ for any $B \in \B(x)$ with $X_{B} \subset X_{B_o}$.
\end{itemize}
Define $L_2(x)$ to be the smallest level $\ell$ such that:
\begin{itemize}[leftmargin=0.5cm]
\item For all $B \in \B_{\geq \ell}(x)$, we have $s(x) \cdot \eta_X(B) \geq 0$.
\item For any $B \in \B_{\leq \ell}(x)$ with $s(x) \cdot \eta_X(B) < 0$, there exists $B' \in \B_{\leq \ell}(x)$ with $X_{B'} \subset X_B$ and $s(x) \cdot \eta_X(B') \geq 0$.
\end{itemize}
Take $L_1(x)$ or $L_2(x)$ to be $\infty$ if no level meets the requirements, or if $\eta(x) = 0$. 
\label{defn:L12}
\end{defn}

Once $\ell$ reaches a critical level, $\PL_\ell(x)$ and thus $\yh_\ell(x)$ behave more predictably, as follows.
\begin{lemma}
Pick any $x \in X$ with $\eta(x) \neq 0$ and let $s(x) \in \{+1,-1\}$ denote its Bayes-optimal label. Then for any level $\ell$ and any time at which $\yh_\ell(x) \neq \bot$:
\begin{enumerate}
\item[(a)] If $\ell \geq L_1(x)$, then $s(x) \in \PL_\ell(x)$ and thus $\yh_\ell(x) \in \{s(x), !\}$. 
\item[(b)] If $\ell \geq L_2(x)$, then $-s(x) \not\in \PL_\ell(x)$ and thus $\yh_\ell(x) \in \{s(x), 0\}$.
\end{enumerate}
\label{lemma:boundary}
\end{lemma}

\subsection{Boundary sets and label complexity}

A common intuition about active learning is that successive queries gradually constrain the possible locations of the decision boundary. Let's consider the state of affairs when all balls at level $\leq \ell - 1$ have been sampled. The known-unknowns from this level are points $x$ with $\yh_{\ell-1}(x) = \, !$; by Lemma~\ref{lemma:boundary}(b), such points must have $L_2(x) \geq \ell$. Focused sampling at level $\ell$ is restricted to balls that contain these points. We will think of this set as the \emph{boundary set} at level $\ell$.
\begin{defn}[Boundary set $\bf \Delta_\ell$]
For any level $\ell$, define the \emph{boundary set} at level $\ell$ to be
\begin{equation}
\Delta_\ell 
= \bigcup_{x \in X: L_2(x) \geq \ell} \bigcup_{B \in \B_{\ell}(x)} X_B 
.
\label{eq:sampling-region}
\end{equation}
\end{defn}
%It can be shown that all focused samples at level $\ell$ must lie in this set.
\begin{lemma}
All focused samples at level $\ell$ lie in $\{z \in \Delta_\ell: T_z \leq \tau_\ell \}$.
\label{lemma:focused}
\end{lemma}

We can now give generic label complexity bounds in terms of $L_1$, $L_2$, and $\Delta_\ell$. These take two equivalent forms: a \emph{global} version that specifies what parts of $X$ are correctly labeled after $m$ queries and a \emph{local} version that specifies the number of queries after which a particular $x$ is correctly labeled. The global version (Theorem~\ref{thm:label-complexity-0}) is in the Appendix; here is the local version.

\begin{thm}[Local query complexity] \label{thm:queryComplexity}
Take $k \geq (192/\gamma^2) \ln (4 |\B|/\delta)$. With probability at least $1-2\delta$, the following holds simultaneously for all $x \in X$. Let $L_1(x)$ and $L_2(x)$ be the critical levels for $x$, as in Definition~\ref{defn:L12}. If $L_2(x) \leq \lg (n/2k)$, let
\[
m_o(x) = 32k \cdot \max\bigg( 2^{L_1(x)}, \ \frac{1}{n} \sum_{\ell=L_1(x)+1}^{L_2(x)} |\Delta_\ell| \, 2^\ell\bigg) .
\]
Once the active learner has made $m_o(x)$ queries, $\yh(x)$ is fixed at the Bayes-optimal label $g^*(x)$.
\label{thm:label-complexity}
\end{thm}
Let's interpret this bound. For a specific $x \in X$, there is no telling how $\yh_\ell(x)$ will behave at levels $\ell < L_1(x)$. But for $\ell \geq L_1(x)$, either $\yh_\ell(x) = \, !$, in which case $x$ is in the uncertainty region, or the label of $x$ is correctly set once and for all, which occurs by level $L_2(x)$. 

We will say that $x$ \emph{reaches level $\ell$} when bias estimates have been obtained for all balls in $\B_{\leq \ell}(x)$ and thus $\yh_\ell(x)$ is set. Recalling that the algorithm alternates between background and focused queries, we show that $m_o(x)/4$ \emph{background} queries suffice for $x$ to reach level $L_1(x)$ and an additional $m_o(x)/4$ \emph{focused} queries take it to level $L_2(x)$, by which time its label is correct.

In particular, $O(k \cdot 2^\ell)$ background queries give us the biases of all balls in $\B_{\leq \ell}$; this is the first term in $m_o(x)$. And there are $O(k \cdot 2^\ell \cdot |\Delta_\ell|/n)$ focused queries at any level $\ell$: the second term.

A footnote: Our algorithm queries points at most once. However, in some applications, a point $x$ can be queried repeatedly, each time producing an independent draw from $\eta(x)$. If \emph{repeat queries} are permissible, $O(1/\gamma^2)$ copies should be made of each point in $X$ before the algorithm is applied; and in this case, points with $|\eta(x)| \geq \gamma$ will all be correctly labeled, eventually.



\section{One-dimensional examples}

To apply Theorem~\ref{thm:label-complexity}, we need bounds on the critical levels $L_1(x)$ and $L_2(x)$ for each $x \in X$, and on the size of the sampling set $\Delta_\ell$ at each level $\ell$. We now derive these in some canonical settings.

\begin{figure}[t]
\begin{center}
  \includegraphics[width=2.5in,height=1.7in]{figures/Monotone.png}
%\hspace{.5in}
\includegraphics[width=2.5in,height=1.7in]{figures/Massart.png}
\end{center}
\caption{(a) The conditional probability function $\eta$ is monotonically increasing; $\eta(x) = -\gamma, 0, \gamma$ at $x = \lambda_L, \lambda, \lambda_R$, respectively. (b) On each subinterval, $\eta$ is either $> \gamma$ or $<-\gamma$.}
\label{fig:oned}
\end{figure}

\subsection{Example: monotonic $\eta$}

Suppose $X$ is an arbitrary set of $n$ points in $\X = [0,1]$ and is labeled according to a conditional probability function $\eta: \X \to [-1,1]$ that is continuous and strictly increasing (Fig.~\ref{fig:oned}(a)). Let $\lambda \in (0,1)$ be the point for which $\eta(\lambda) = 0$ and let $\lambda_L, \lambda_R$ be the points for which $\eta(\lambda_L) = -\gamma$ and $\eta(\lambda_R) = \gamma$. Thus we are not required to label points in the interval $(\lambda_L, \lambda_R)$. 




%\begin{figure}
%\begin{center}
%\includegraphics[width=3in]{oned-monotonic.pdf}
%\end{center}
%\caption{The conditional probability function $\eta$ is monotonically increasing. The values $x = %\lambda_L, \lambda, \lambda_R$ have $\eta(x) = -\gamma, 0, \gamma$ respectively.}
%\label{fig:oned-monotonic}
%\end{figure}


Let $\B$ consist of all closed intervals. Although there are infinitely many of these, we need only consider those whose endpoints lie in $X$, so effectively $|\B| = O(n^2)$.

The critical levels are easy to bound in this setting. Let $n^- = |[0,\lambda_L] \cap X|$  be the number of points to the left of $\lambda_L$ and $n^+ = |[\lambda_R,1] \cap X|$ the number of points to the right of $\lambda_R$. Then 
%\begin{align*}
%L_1(x)
%&\leq
%\left\{
%\begin{array}{ll}
%\lg (n/n^+) & \mbox{if $x \geq \lambda_R$} \\
%\lg (n/n^-) & \mbox{if $x \leq \lambda_L$}
%\end{array}
%\right.
%\\
%L_2(x)
%&\leq
%\lg (n/r(x))
%\end{align*}
$$
L_1(x)
\leq
\left\{
\begin{array}{ll}
\lg (n/n^+) & \mbox{if $x \geq \lambda_R$} \\
\lg (n/n^-) & \mbox{if $x \leq \lambda_L$}
\end{array}
\right.
\mbox{\ \ \ \ and \ \ \ }
L_2(x)
\leq
\lg (n/r(x))
$$
where $r(x)$ is the number of data points between $x$ and $\lambda$, counting $x$ as well.
%\begin{itemize}
%\item $L_1(x) \leq \lg (n/n^+)$ if $x \geq \lambda_R$ and $L_1(x) \leq \lg (n/n^-)$ if %$x \leq \lambda_L$, and
%\item $L_2(x) \leq \lg (n/r(x))$, 
%\end{itemize}

A counting argument then shows that the boundary set shrinks exponentially as the level increases, $|\Delta_\ell| \leq 4n/2^\ell$, yielding the following label complexity. 
\begin{thm}
Define $r^+ = \min \{r(x): x \in X \cap [\lambda_R,1]\}$ and $r^- = \min \{r(x): x \in X \cap [0,\lambda_L]\}$. Pick any $0 < \delta < 1$. Suppose we run the algorithm of Fig.~\ref{alg:main} with $k = O((\log (n/\delta))/\gamma^2)$ and that $\min(r^+, r^-) \geq 2k$.  Take any 
$$ m \geq 64k \cdot \max \left( \frac{n}{\min(n^+,n^-)}, \ 2 \lg \frac{\min(n^+,n^-)}{\min(r^+,r^-)} \right) .$$
With probability $\geq 1-\delta$, after $m$ queries the algorithm correctly labels all $x \in X$ with $|\eta(x)| \geq \gamma$.
\label{thm:oned-monotonic}
\end{thm}
If $n^+, n^- = \Theta(n)$, that is, a constant fraction of the points lie to the left of $\lambda_L$ and to the right of $\lambda_R$, then the label complexity is $O(\log^2 n)$. If in addition $r^+, r^- = \Theta(n)$, that is, a constant fraction of the points lie between $\lambda_L, \lambda_R$ and the decision boundary, the label complexity is $O(\log n)$. 

\subsection{Example: Massart noise}
\label{sec:example-massart}

In Section~\ref{sec:oned-massart}, we develop another one-dimensional example, in which $\X$ consists of several pieces, such that on each piece $\eta$ is either entirely above $\gamma$ or entirely below $-\gamma$ (Fig.~\ref{fig:oned}(b)). Once again, a logarithmic label complexity is obtained. (Details of the proof are in Section~(\ref{sec:oned-massart})

\begin{figure}
\begin{center}
\includegraphics[width=3in]{figures/MassartExperiment5.png}
\end{center}
\caption{{\bf Experimental evidence:} This figure compares the
  performance of passive sampling alone to that of the active learning
  algorithm. The X axis corresponds to the number of queries made and the
  Y axis represents the difference between the error achieved and the
  Bayes error.}
\label{fig:Experiments}
\end{figure}

\section{Experiments}
\label{sec:experiment}
We describe an experiment designed to evaluate our algorithm.
We use a simple 1D problem, described in Section~\ref{sec:example-massart}.
Although this problem is simple, it is outside the scope of most non-parametric active learning algorithms, which assume smoothness of the conditional probability.

We compare two algorithms, the "active" agorithm described here, and the "passive" algorithm which uses only "background queries". We denote the difference between the error of the generated classifier and that of the Bayes optimal classifier by $\epsilon$. From the theory, we expect the query complexity of the passive algorithm to be $O(\frac{1}{\epsilon})$ and of the active algorithm to be $O(\log (\frac{1}{\epsilon})$. 


 For our experiment we used a sample of 10,000 unlabeled points, we repeated each of our experiments 20 times. In Figure~\ref{fig:Experiments} we show the average performance of these experiments. Note the horizontal line segments marked "active vs. passive <error>", these compare the number of queries required by the active and the passive algorithms for different accuracies
 These numbers are repeated in the following table.
\begin{center}
\begin{tabular}{|c|rr|}
\hline
err-opt &  Active&  Passive \\
\hline
0.02  &    320 &     517 \\
0.01 &     429 &    1035 \\
0.005 &678 &    2045 \\
\hline
\end{tabular}
\end{center}

Considering the columns for the Active and Passive algorithms respectively, we see that the experimental query complexities concur with the theoretical predictions.

\section{Future Directions}
Our algorithm is generic, while we used euclidean balls for clarity,
any sets can be used. While this generality is attractive, it leaves
open the question of what are good ways to define the collection of
regions. Another aspect of this generality is that the computational
complexity of the algorithm is high. Future work will be directed
towards characterizing collections of regions that give both low query
complexity and low computational complexity.


%\input{active-knn.bbl}
\bibliography{refs}
\bibliographystyle{plain}

\iffalse
\documentclass[twoside]{article}

\usepackage{aistats2024}
\usepackage{enumitem}
\usepackage{multicol}
\usepackage{times}
\usepackage{amssymb}
\usepackage{graphicx}

\def\R{{\mathbb{R}}}
\def\pr{{\rm Pr}}
\def\E{{\mathbb E}}
\def\X{{\mathcal X}}
\def\Y{{\mathcal Y}}
\def\H{{\mathcal H}}
\def\G{{\mathcal G}}
\def\B{{\mathcal B}}
\def\yh{{\widehat{y}}}
\def\bias{{\rm bias}}
\def\supp{{\rm supp}}
\def\dist{{\rm dist}}
\def\sign{{\rm sign}}
\def\vol{{\rm vol}}
\def\PL{{\mbox{\rm PL}}}

\newtheorem{thm}{Theorem}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
%\newtheorem{claim}[thm]{Claim}
\newtheorem{defn}[thm]{Definition}
\newtheorem{assump}{Assumption}
\newenvironment{proof}{\noindent {\sc Proof:}}{$\Box$ \medskip}

\newcommand{\comment}[2]{ {\bf #1 :} #2}
\newcommand{\yoav}[1]{\comment{Yoav}{\em #1}}
\newcommand{\sanjoy}[1]{\comment{blue}{Sanjoy}{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}

% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%
%\usepackage[accepted]{aistats2024}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
%\usepackage[round]{natbib}
%\renewcommand{\bibname}{References}
%\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\fi

\appendix
\onecolumn
\begin{center}
\LARGE{Non-parametric active learning without smoothness\\
Supplementary Materials}
\end{center}

%%%%%%%%%%%%
\section{Analysis Details: Finite Population setting}
\label{sec:FinitePopulationProofs}

\subsection{Technicalities: finite population setting}

\subsubsection{Large deviation bounds for the finite population setting}

\begin{lemma}
Fix a confidence parameter $0 < \delta < 1$ and a positive integer $k \geq 6 \ln (4/\delta)$. 

Let $x_1, \ldots, x_m$ be any collection of points. Suppose that the labels $Y_i \in \{-1,1\}$ of these points are independent, with $\E Y_i = \eta(x_i)$. Define
$$ \eta_o = \frac{1}{m} \left( \eta(x_1) + \cdots + \eta(x_m) \right) .$$
Now consider the following estimator $Z$ of this quantity:
\begin{itemize}
\item Each $x_i$ is chosen with probability $q > 0$, independently. Let $N$ be the number of selected points.
\item If $N > 0$, the labels $Y_i$ of the selected points are obtained, and $Z$ is their average.
\end{itemize}
If $qm \geq k + \sqrt{6k \ln (4/\delta)}$, with probability at least $1-\delta$, 
\begin{enumerate}
\item[(a)] $N \geq k$, and 
\item[(b)] $| Z - \eta_o| < \sqrt{(48/k) \ln (4/\delta)}$.
\end{enumerate}
\label{lemma:large-deviation-discrete}
\end{lemma}
\begin{proof}
Let's start with (a). Define $c = \sqrt{3 \ln (4/\delta)}$. We'll take $qm = k + \sqrt{6k \ln (4/\delta)} = k + c \sqrt{2k}$ since this is the worst case. By assumption, $k \geq 2c^2$ and thus $qm \leq 2k$.

Now, $N$ has a binomial($m,q$) distribution. By a multiplicative Chernoff bound, for $0 < \epsilon < 1$, we have
\begin{align*}
\pr(N \geq qm(1+\epsilon)) &\leq e^{-qm\epsilon^2/3} \\
\pr(N \leq qm(1-\epsilon)) &\leq e^{-qm\epsilon^2/2}
\end{align*}
Take $\epsilon = c/\sqrt{qm}$; by the lower bound on $k$, we have $\epsilon \leq 1/2$. Recalling the choice of $c$, we see that with probability at least $1-\delta/2$, we get 
$$(1-\epsilon) qm < N < (1+\epsilon) qm .$$
The lower bound implies $N > qm(1-\epsilon) = qm - c\sqrt{qm} = k + c \sqrt{2k} - c \sqrt{qm} \geq k$.

For (b), define $C_1, \ldots, C_m \in \{0,1\}$ as random variables indicating whether the corresponding points were selected; that is, $C_i = {\bf 1}(\mbox{$x_i$ was selected})$. The sum of the obtained labels is then $S = C_1 Y_1 + \cdots + C_m Y_m$. Notice that these $C_iY_i \in \{-1,0,1\}$ are independent with $\E[C_iY_i] = q \eta(x_i)$ and $\E[(C_iY_i)^2] = \E[C_i] = q$. Thus their sum $S$ has expectation
$$ \E [S] = \sum_{i=1}^m \E[C_i Y_i] = \sum_{i=1}^m q \eta(x_i) = qm \eta_o $$
and variance
$$ \mbox{var}(S) = \sum_{i=1}^m \mbox{var}(C_iY_i) \leq qm .$$
We can bound the concentration of $S$ around its expected value using Bernstein's inequality, by which
$$ \pr(|S - \E[S]| \geq t) \leq 2 \exp \left( - \frac{t^2}{2(\mbox{var}(S) + t/3)} \right) .$$
Using $t = \epsilon qm$, we then have that $|S - qm \eta_o| \leq \epsilon qm$ with probability at least $1-\delta/2$.

Combining with the high-probability bound on $N$ above, we get
$$ \frac{qm \eta_o - \epsilon qm}{qm(1+\epsilon)} < \frac{S}{N} < \frac{qm \eta_o + \epsilon qm}{qm(1-\epsilon)},$$
whereupon (recalling $Z = S/N$)
$$ 
\eta_o \left( \frac{1}{1+\epsilon} -1 \right) - \frac{\epsilon}{1+\epsilon} < Z - \eta_o < \eta_o \left( \frac{1}{1-\epsilon} - 1 \right) + \frac{\epsilon}{1-\epsilon} .$$
Since $|\eta_o| \leq 1$,
$$ |Z - \eta_o | < \max \left( \frac{2\epsilon}{1+\epsilon}, \frac{2\epsilon}{1-\epsilon} \right)
\leq 
4 \epsilon,$$
as claimed.  
\end{proof}


\subsubsection{Accuracy of bias estimates: Proof of Lemma~\ref{lemma:accurate-bias-estimate}}

We will use Lemma~\ref{lemma:large-deviation-discrete} to obtain a uniform guarantee on the bias estimates for all balls $B \in \B$.

Recall that each point $x \in X$ gets a label $Y \in \{-1,+1\}$ according to the distribution
$$ \eta(x) = \E[y | x].$$ 
For any $B \in \B$ with $X_B \neq \emptyset$, let $\eta_X(B)$ be the average $\eta$-value of the points in $B$, that is,
$$ \eta_X(B) = \frac{1}{|X_B|} \sum_{x \in X_B} \eta(x) .$$

For any $B \in \B_\ell$, define its \emph{query-set} to be $\Gamma(B) = \{z \in X_B: T_z \leq \tau_\ell\}$, where the sampling threshold $\tau_\ell$ for level $\ell$ is defined as:
\begin{equation}
\tau_\ell = \min \left( \frac{2^{\ell+2}k}{n}, \ 1 \right)
\end{equation}
for some $k$. We base our bias-estimate for $B$ on the labels of points in $\Gamma(B)$.

For what follows, define
\begin{equation*}
c = \sqrt{3 \ln \frac{4|\B|}{\delta}} .
\end{equation*}

\begin{lemma}
Suppose $k \geq 2c^2$. With probability at least $1-\delta$, the following is true for all $B \in \B$ with $|X_B| \geq k$:
\begin{enumerate}
\item[(a)] The query set $\Gamma(B)$ has size at least $k$.
\item[(b)] The average label on $\Gamma(B)$, call it $\widehat{\eta}(B)$, satisfies
$$ \left| \widehat{\eta}(B) - \eta_X(B) \right| \leq \frac{4c}{\sqrt{k}}.$$
\end{enumerate}
\label{lemma:large-deviation-bounds}
\end{lemma}
\begin{proof}
Pick $B \in \B$. There are two cases to consider.

Case 1: $|X_B| \geq 2k$. The choice of $\tau_\ell$ then ensures $|X_B| \tau_\ell \geq 2k$, so that $|\widehat{\eta}(B) - \eta_X(B)|$ can be bounded by applying Lemma~\ref{lemma:large-deviation-discrete} to the points $X_B$ with sampling probability $q = \tau_\ell$.

Case 2: $k \leq |X_B| < 2k$. In this case, $B$ lies at a level $\ell$ for which $\tau_\ell = 1$. Lemma~\ref{lemma:large-deviation-discrete} does not apply directly, but its conclusion still holds. In particular, the query-set $\Gamma(B)$ is all of $X_B$, and the same Bernstein bound from the proof of Lemma~\ref{lemma:large-deviation-discrete} can be again be applied.

To complete the proof, we take a union bound over all $B \in \B$.
\end{proof}

The following corollary of Lemma~\ref{lemma:large-deviation-bounds} is immediate.
\begin{cor}
Suppose that $k \geq (8c/\gamma)^2$. For each $B \in \B$, let $\widehat{\eta}(B)$ be the average label on the query-set $\Gamma(B)$, and define the bias-estimate $\yh(B)$ as follows:
$$ \yh(B)
= 
\left\{
\begin{array}{ll}
\sign(\widehat{\eta}(B)) & \mbox{if $|\widehat{\eta}(B)| \geq \gamma/2$} \\
0 & \mbox{otherwise}
\end{array}
\right.
$$
With probability $\geq 1-\delta$, all bias estimates $\yh(B)$, for $B \in \B$, are $\gamma$-accurate.
\label{cor:accurate-bias-estimates}
\end{cor}
\begin{proof}
By the choice of $k$, we have from Lemma~\ref{lemma:large-deviation-bounds} that 
$| \widehat{\eta}(B) - \eta_X(B) | < \gamma/2$
for all $B \in \B$.
\end{proof}

\subsection{Critical levels: Proof of Lemma~\ref{lemma:boundary}}

For part (a), note that some $B_o \in \B_{L_1(x)}(x)$ has significant bias (that is, bias $\geq \gamma$) towards the correct label $s(x)$, as does any ball $B \in \B(x)$ contained within it. For $\ell \geq L_1(x)$, the set $\{B \in \B_{\leq \ell}(x): X_{B} \subseteq X_{B_o}\}$ is nonempty (it contains $B_o$); pick any minimal ball $B$ within it. Then $\yh(B) = s(x)$ by the $\gamma$-accuracy of bias estimates (Lemma~\ref{lemma:accurate-bias-estimate}) and thus $s(x) \in \PL_\ell(x)$.

For (b), take any $\ell \geq L_2(x)$. Consider any $B \in \B_{\leq \ell}(x)$ for which $s(x) \cdot \eta_X(B) < 0$. By definition of $L_2(x)$, this $B$ must lie in $B_{< L_2(x)}(x)$, and moreover there must exist $B' \in \B_{\leq L_2(x)}(x) \subset \B_{\leq \ell}(x)$ with $X_{B'} \subset X_B$ and $s(x) \cdot \eta_X(B') \geq 0$. Thus, any minimal $B \in \B_{\leq \ell}(x)$ has bias $\geq 0$ towards the correct label $s(x)$, whereupon $\yh(B) \in \{0, s(x)\}$ by the $\gamma$-accuracy of bias estimates. Therefore, $-s(x) \not\in \PL_\ell(x)$.

\subsection{The region of focused sampling: Proof of Lemma~\ref{lemma:focused}}

Let $\overline{U}_\ell$ denote the set of all points that are ever (in any round of sampling) in the uncertainty region at level $\ell$. From Lemma~\ref{lemma:boundary}(b), we see that any $x$ with $L_2(x) < \ell$ has $\yh_{\ell-1}(x) \neq \ !$ and thus never makes it into the uncertainty region at level $\ell$. In short,
\begin{equation}
\overline{U}_{\ell} \subset \{x \in X: L_2(x) \geq \ell\}.
\label{eq:uncertainty}
\end{equation}

We see from the {\tt Focused-query} subroutine (Figure~\ref{alg:sampling}) that all focused samples at level $\ell$ lie in
$$
\bigcup_{x \in \overline{U}_\ell} \bigcup_{B \in \B_\ell(x)} \{z \in X_B: T_z \leq \tau_\ell \}
\ 
\subset 
\ 
\bigcup_{x \in X: L_2(x) \geq \ell} \bigcup_{B \in \B_\ell(x)} \{z \in X_B: T_z \leq \tau_\ell \}
\ 
=
\ 
\{z \in \Delta_\ell: T_z \leq \tau_\ell \}.
$$

\subsection{A generic label complexity bound}

We start by showing that various subsets of interest contain roughly the expected number of points at each level.
\begin{lemma}
With probability at least $1-2(\lg (n/k))e^{-k/3}$, the following hold for all levels $0 \leq \ell \leq \lg (n/2k)$.
\begin{enumerate}
\item[(a)] $|\{x \in X: T_x \leq \tau_\ell\}| < 2 n \tau_\ell$.
\item[(b)] If $\Delta_\ell \neq \emptyset$ then $|\{x \in \Delta_{\ell}: T_x \leq \tau_\ell\}| < 2 |\Delta_\ell| \tau_\ell$.
\end{enumerate}
\label{lemma:level-distribution}
\end{lemma}
\begin{proof} 
Pick any subset $S \subset X$ and let $m = |S|$. Then $|\{x \in S: T_x \leq \tau_\ell\}|$ has a $\mbox{binomial}(m, \tau_\ell)$ distribution with expectation $m \tau_\ell$. The probability that it is greater than or equal to twice its expected value is, by a multiplicative Chernoff bound, at most $e^{-m \tau_{\ell}/3}$, which is $\leq e^{-k/3}$ as long as $m \tau_\ell \geq k$.

Both parts follow from this principle; and we take a union bound over all $\lg (n/k)$ levels. For (b), we need to check that $|\Delta_\ell| \tau_\ell \geq k$. To see this, observe from the definition (\ref{eq:sampling-region}) of $\Delta_\ell$ that if it is non-empty, then it contains $X_B$ for at least one ball $B \in \B_\ell$, and every such ball has at least $n/2^{\ell+1}$ points. Combining this with the definition $\tau_\ell = \min(2^{\ell+2}k/n, 1)$ yields $|\Delta_\ell| \tau_\ell \geq k$.
\end{proof}

Theorem~\ref{thm:label-complexity} is a restated version of the following.
\begin{thm}
Suppose the active learning algorithm makes $0 < m \leq n$ queries. Then all points $x \in X$ with $L_1(x) \leq \ell_1$ and $L_2(x) \leq \ell_2$ will get Bayes-optimal labels $\yh(x) = g^*(x)$, where
$$ \ell_1 = \left\lfloor \lg \frac{m}{32k} \right\rfloor $$
and $\ell_2$ is the largest integer $\leq \lg (n/2k)$ such that
$$ \sum_{\ell = \ell_1 + 1}^{\ell_2} |\Delta_{\ell}| \, \tau_{\ell} \ < \ \frac{m}{8} .$$
\label{thm:label-complexity-0}
\end{thm}

\begin{proof}
Denote the first $m/2$ queries by {\it phase one} and the second $m/2$ by {\it phase two}. We will analyze the effect of background sampling in phase one and focused sampling in phase two. We start with the former.

Of the $m/2$ queries in phase one, at least $m/4$ will be background samples. Therefore the $m/4$ points with lowest $T_x$ values are guaranteed to be queried.

Now, for $\ell_1$ as defined, we have that $\tau_{\ell_1} \leq m/8n$ and thus from Lemma~\ref{lemma:level-distribution}(a) that at most $m/4$ points in $X$ satisfy $T_x \leq \tau_{\ell_1}$. Therefore all such points are queried in phase one, and all label-estimates $\{\yh_{\ell_1}(x): x \in X\}$ are set.

It then follows from Lemma~\ref{lemma:boundary}(a) that the following hold for any $x \in X$ with $L_1(x) \leq \ell_1$:
\begin{enumerate}
\item[(a)] By the end of phase one, $\yh_{\ell_1}(x) \in \{g^*(x), !\}$.
\item[(b)] For any $\ell > \ell_1$, when $\yh_\ell(x)$ becomes available, it lies in $\{g^*(x), !\}$.
\item[(c)] If $x$ ever leaves the combined uncertainty region $U = \cup_\ell U_\ell$ during phase two, then its final label as defined in (\ref{eq:final-label}) is henceforth always $\yh(x) = g^*(x)$.
\end{enumerate}

Now let's move on to phase two. Let $A = \{x \in X: L_1(x) \leq \ell_1, L_2(x) \leq \ell_2\}$. We will show that every point in $A$ must leave the uncertainty region $U$ at some time during phase two. From (c), we can conclude that all these points have their final labels set correctly, once and for all.

We will break the argument into two cases. 

{\it Case 1:} Fewer than $m/4$ focused queries are made in phase two. This can only happen if some round of sampling has an empty uncertainty region, meaning that all of $A$ has left $U$ at that point.

{\it Case 2:} A full $m/4$ focused queries are made in phase two. By the analysis of phase one, none of these queries can be at level $\leq \ell_1$ and by Lemma~\ref{lemma:focused}, the total number of possible focused queries at levels $\ell_1+1$ through $\ell_2$ inclusive is at most
$$\sum_{\ell=\ell_1+1}^{\ell_2} |\{z \in \Delta_{\ell}: T_z \leq \tau_\ell \}| 
\ \leq \ 
\sum_{\ell=\ell_1+1}^{\ell_2} 2 |\Delta_{\ell}| \tau_\ell
\ < \ 
\frac{m}{4} ,
$$
where the first inequality is from Lemma~\ref{lemma:level-distribution}(b). Thus at least one query in phase two must be at level $\ell_2 +1$. When this query is made, every $U_\ell$ with $\ell \leq \ell_2$ must be empty and thus all of $A$ must have left the uncertainty region; recall from (\ref{eq:uncertainty}) that no $x \in A$ can be part of $U_\ell$ for $\ell > \ell_2 \geq L_2(x)$. 

Thus every $x \in A$ must leave the uncertainty region at some point in phase two, and their final labels are subsequently set correctly.
\end{proof}


\subsection{One-dimensional monotonic $\eta$: Proof of Theorem~\ref{thm:oned-monotonic}}

We begin by bounding the critical levels $L_1$ and $L_2$ for points in $X$.
\begin{lemma}
Pick any $x \in [0, 1]$.
\begin{enumerate}
\item[(a)] Define $n^- = |[0,\lambda_L] \cap X|$ and $n^+ = |[\lambda_R,1] \cap X|$. Then
$$
L_1(x)
\leq
\left\{
\begin{array}{ll}
\lg (n/n^+) & \mbox{if $x \geq \lambda_R$} \\
\lg (n/n^-) & \mbox{if $x \leq \lambda_L$}
\end{array}
\right.
$$
\item[(b)] Let $r(x)$ be the number of points between $x$ and the boundary point $\lambda$, counting $x$ itself. That is, $r(x) = |[x,\lambda) \cap X|$ if $x < \lambda$, or $|(\lambda, x] \cap X|$ if $x > \lambda$. Then $L_2(x) \leq \lg (n/r(x))$.
\end{enumerate}
\label{lemma:oned-monotonic-L}
\end{lemma}
\begin{proof}
For (a), take any $x \geq \lambda_R$ (the other case is similar). The interval $B = [\lambda,1]$ lies in $\B_\ell(x)$ for $\ell = \lceil \lg (n/n^+) - 1 \rceil$ and has $\eta_X(B) \geq \gamma$. Furthermore, any $B' \subset B$ also has $\eta_X(B') \geq \gamma$. 

For (b), take $x \geq \lambda_R$ and $\ell \geq \lg (n/r(x))$. Any $B \in \B_\ell(x)$ contains $< n/2^\ell \leq r(x)$ points and thus cannot possibly extend to the other side of the boundary. It follows that every $B \in \B_{\geq \ell}(x)$ has $\eta_X(B) \geq 0$. Moreover, any interval $B'$ that does extend to the other side of the boundary contains $B' \in \B_{\ell}(x)$ that is entirely on the same side as $x$.
\end{proof}

We can now bound the size of the query region at each level and find that it shrinks exponentially with $\ell$.
\begin{lemma}
For any $\ell \geq 0$, let $\Delta_\ell$ denote the focused querying region at level $\ell$, as defined in (\ref{eq:sampling-region}). Then $|\Delta_\ell| \leq 4n/2^\ell$.
\label{lemma:oned-monotonic-query-region}
\end{lemma}

\begin{proof}
We have
\begin{align*}
\Delta_\ell 
&= \bigcup_{x \in X: L_2(x) \geq \ell} \bigcup_{B \in \B_{\ell}(x)} (B \cap X) \\
&\subset \bigcup_{x \in X: r(x) \leq n/2^\ell} \bigcup_{B \ni x: |B \cap X| < n/2^\ell} (B \cap X) 
.
\end{align*}
This includes at most $n/2^{\ell-1}$ points from $X$ on either side of $\lambda$. \end{proof}


We are now ready for the proof of Theorem~\ref{thm:oned-monotonic}.

Setting $k$ to $O((1/\gamma^2) \ln (n/\delta))$ satisfies the requirements of Theorem~\ref{thm:label-complexity}. Here we are using the fact that although $\B$ is infinite, we need only consider $O(n^2)$ distinct intervals since $|X| = n$.

First observe that for any $\ell_1 \leq \ell_2$, we have from Lemma~\ref{lemma:oned-monotonic-query-region} that
$$ \sum_{\ell=\ell_1+1}^{\ell_2} |\Delta_\ell| \tau_\ell 
\ \leq \ \sum_{\ell=\ell_1+1}^{\ell_2} \frac{4n}{2^\ell} \cdot \frac{2^{\ell+2} k}{n} 
\ = \ 16k(\ell_2-\ell_1).$$
Now, let's define
$$ \ell_1 = \left\lfloor \lg \frac{m}{32k} \right\rfloor, 
\ \ \ \ell_2 = \min \left( \ell_1 + \frac{m}{128k}, \ \ \lg \frac{n}{2k} \right).$$
Then for any $x \in [0, \lambda_L] \cup [\lambda_R,1]$, we have
$$ \ell_1 = \left\lfloor \lg \frac{m}{32k} \right\rfloor 
\ \geq \ 
\lg \frac{m}{64k}
\ \geq \ 
\lg \frac{n}{\min(n^+,n^-)}
\ \geq \ 
L_1(x)$$
and, if $\min(r^+, r^-) \geq 2k$,
\begin{align*}
\ell_2 
\ = \ \ell_1 + \frac{m}{128k} 
&\geq \lg \frac{m}{64k} + \lg \frac{\min(n^+,n^-)}{\min(r^+,r^-)} \\
&\geq \ \lg \frac{m}{64k} + \lg \frac{64kn/m}{\min(r^+,r^-)} 
\ = \ \lg \frac{n}{\min(r^+,r^-)}
\ \geq \  
L_2(x).
\end{align*}
We get the algorithmic guarantee by applying Theorem~\ref{thm:label-complexity}. 

\subsection{One-dimensional data with Massart noise}
\label{sec:oned-massart}

We now turn to another one-dimensional setting. Once again, $X$ consists of $n$ arbitrarily-placed points in $\X = [0,1]$. This time, however, they are labeled according to a conditional probability function $\eta: \X \to [-1,1]$ that satisfies the Massart noise condition:
\begin{itemize}
\item There are $p$ disjoint open intervals $I_1, \ldots, I_p$, such that $\X$ is (the closure of) their union, and
\item for each $j$, either $\eta(x) > \gamma$ for all $x \in I_j$ or $\eta(x) < -\gamma$ for all $x \in I_j$. In the first case, we write $s(I_j) = +1$ and in the second case, $s(I_j) = -1$.
\end{itemize}
Here $0 < \gamma < 1$ is some constant. See Figure~\ref{fig:oned}(b) for an illustrative example. For concreteness, the intervals $I_j$ can be written in the form $(\lambda_{j-1}, \lambda_j)$, where
$0 = \lambda_0 < \lambda_1 < \cdots < \lambda_{p-1} < \lambda_p = 1 .$
Here $\lambda_1, \ldots, \lambda_{p-1}$ are the \emph{boundary points} between intervals.


We will take $\B$ to consist of all open intervals of $[0,1]$, with $\B(x)$ denoting intervals that contain point $x$. 

We begin with bounds on the $L_1$ and $L_2$ levels for each point.
\begin{lemma}
Pick any $x \in \X$; suppose $x \in I_j$.
\begin{enumerate}
\item[(a)] Let $n_j = |X \cap I_j|$. Then $L_1(x) \leq \lg (n/n_j)$.
\item[(b)] Let $r(x)$ be the minimum number of data points that lie between $x$ and a boundary point, counting $x$ as well; this is either the number of points in the left-interval $(\lambda_{j-1},x]$ (if $j > 1$) or the right-interval $[x, \lambda_j)$ (if $j < p$), whichever is smaller. Then $L_2(x) \leq \lg (n/r(x))$.
\end{enumerate}
\label{lemma:oned-massart-L}
\end{lemma}
\begin{proof}
For (a), notice first that $I_j \in \B_{\ell}(x)$ for $\ell = \lceil (\lg n/n_j) - 1 \rceil$. Moreover, $s(I_j) \cdot \eta_X(I_j) > \gamma$. Thus $I_j$ belongs to $\B_\ell(x)$ and is strongly biased towards the correct label. This strong bias also holds for any subset of $I_j$. 

For (b), consider any $\ell \geq \lg (n/r(x))$. Any $B \in \B(x)$ with $\mbox{sign}(\eta_X(B)) \neq s(I_j)$ must contain either the entire left-interval $(\lambda_{j-1},x]$ or the entire right-interval $[x, \lambda_j)$, and thus has at least $r(x)$ points, which means that it is too large to be in $\B_\ell$. Thus all intervals $B \in \B_{\geq \ell}(x)$ have $s(I_j) \cdot \eta_X(B) > 0$. Also, for any interval $B \in \B_{<\ell}(x)$ there is some $B' \in \B_{\ell}(x)$ that is strictly contained within it.
\end{proof}


With $L_1(x)$ and $L_2(x)$ under control, it is easy to bound the size of the focused query region $\Delta_\ell$ at each level.
\begin{lemma}
For any $\ell \geq 0$, let $\Delta_\ell$ denote the focused querying region at level $\ell$, as defined in (\ref{eq:sampling-region}). Then $|\Delta_\ell| \leq (p-1)n/2^{\ell-2}$.
\label{lemma:oned-massart-query-region}
\end{lemma}

\begin{proof}
We have
\begin{align*}
\Delta_\ell 
&= \bigcup_{x \in X: L_2(x) \geq \ell} \bigcup_{B \in \B_{\ell}(x)} (B \cap X) \\
&\subset \bigcup_{x \in X: r(x) \leq n/2^\ell} \bigcup_{B \ni x: |B \cap X| < n/2^\ell} (B \cap X) 
.
\end{align*}
This includes at most $n/2^{\ell-1}$ points from $X$ on either side of each boundary point $\lambda_j$.
\end{proof}

Notice that $|\Delta_\ell|$ shrinks exponentially with $\ell$. With $L_1$, $L_2$, and $|\Delta_\ell|$ values in place, Theorem~\ref{thm:label-complexity} can be applied directly to give the following label complexity bound.

\begin{thm}
Pick any $0 < \epsilon, \delta < 1$. Suppose we run the algorithm of Figure~\ref{alg:main} with $k = O((1/\gamma^2) \ln (n/\delta))$. With probability at least $1-\delta$, after making
$$ O \left( \frac{p-1}{\gamma^2} \ln \frac{p-1}{\epsilon} \ln \frac{n}{\delta} \right) $$
queries, the algorithm will assign the correct label $g^*(x)$ to at least $1-\epsilon$ fraction of $X$, except possibly the $2k$ points of either side of each boundary point.
\label{thm:oned-massart}
\end{thm}

\begin{proof}
Setting $k$ to $O((1/\gamma^2) \ln (n/\delta))$ satisfies the requirements of Theorem~\ref{thm:label-complexity}. Here we are using the fact that although $\B$ is infinite, we need only consider $O(n^2)$ distinct intervals since $|X| = n$.

Next, using Lemma~\ref{lemma:oned-massart-query-region}, we have that for any integers $0 \leq \ell_1 < \ell_2$,
$$ \sum_{\ell = \ell_1+1}^{\ell_2} |\Delta_\ell| \tau_\ell
\ \leq \ 
\sum_{\ell = \ell_1+1}^{\ell_2} \frac{(p-1) n}{2^{\ell-2}} \cdot \frac{2^{\ell+2}k}{n}
\ = \ 
16(p-1)k (\ell_2 - \ell_1).
$$
We can then apply Theorem~\ref{thm:label-complexity} to conclude that $m$ query points are enough to correctly classify all $x \in X$ with $L_1(x) \leq \ell_1$ and $L_2(x) \leq \ell_2$, for
\begin{align*}
\ell_1
&= 
\left\lfloor \lg \frac{m}{32k} \right\rfloor \\
\ell_2
&=
\min \left( \ell_1 + \frac{m}{128 k(p-1)}, \ \lg \frac{n}{2k} \right)
\end{align*}
Using Lemma~\ref{lemma:oned-massart-L}, we have that in every target interval $I_j$ with $n_j/n = \Omega(k/m)$, all but $n \cdot 2^{-\ell_2-1}$ points will be correctly classified. For large enough $m$, this means that the fraction of misclassified or unclassified points in $X$ will be at most $\epsilon$ after $O(k(p-1) \log ((p-1)/\epsilon))$ queries, apart from the $2k$ points nearest the boundaries, which will remain unclassified.
\end{proof}

The $2k$ points nearest each boundary cannot be labeled by our algorithm with any certainty because they lie in intervals with a strongly positive bias as well as in intervals with a strongly negative bias. This qualification would be removed if were allowed to make multiple queries to each point, because in that case we would include $O(k)$ copies of each point, as explained earlier.

\section{Analysis: distributional setting}
\label{sec:continuous}

We now turn to a setting where the points $X$ are drawn from some distribution $\mu$ on a topological space $\X$. For the same algorithm, we bound label complexity in terms of properties of $\mu$ and $\eta$. 

\subsection{A sufficiently-rich collection of balls}

Let $\B$ be a finite collection of subsets of $\X$; we spell out a basic property we need it to possess.

\begin{defn}
We say $\B$ is \emph{$k$-conforming} with respect to $X$ if 
\begin{itemize}
\item ({\bf Richness}) For any $x \in X$, there exists $B \in \B_0(x)$. Moreover, for any level $0 \leq \ell \leq \lg (n/k)$ and any $B \in \B_\ell(x)$, there exists $B' \in \B_{\ell+1}(x)$ such that $X_{B'} \subset X_B$.
\item ({\bf Fidelity}) $\mu(B)/2 \leq |X_B|/n \leq 2\mu(B)$ for all $B \in \B$ with $|X_B| \geq k$.
\end{itemize}
\label{defn:conformity}
\end{defn}
When $\B$ consists of Euclidean balls in $\R^d$, made finite by restricting to $X$, the first property is immediate and the second holds with high probability (Lemma~\ref{lemma:ball-size-bounds}). We also envisage scenarios where $\B$ is constructed using $X$, for instance from the cells of multiple spatial partition trees.

Henceforth assume $\B$ satisfies this property.  We ignore $B$ with $|X_B| < k$, thereby restricting attention to levels $0 \leq \ell < \lg (n/k)$.

\subsection{A probabilistic notion of distance}

A crucial step in the analysis is to introduce a distance function based on $\mu$: the distance from a point $x$ to a set $S$ is the probability mass of the smallest ball that contains $x$ and touches $S$.
\begin{defn}
For any $x \in \X$ and $S \subset \X$, define
$\dist(x,S) = \inf \{\mu(B): B \in \B(x), B \cap S \neq \emptyset\} .$
\label{defn:prob-dist}
\end{defn}
We will see that $L_2(x)$ can be bounded in terms of such distances. In what follows, for $s \in \{-1,0,+1\}$, take $\X^s$ to denote $\{x \in \X: \sign(\eta(x)) = s\}$.
\begin{lemma}
For any $x \in \X$, let $s(x) = \sign(\eta(x))$. If $\dist(x, \X^{-s(x)}) \geq 2k/n$, then
$L_2(x) \leq \bigg\lceil \lg \frac{1}{\dist(x, \X^{-s(x)})} \bigg\rceil + 1$.
\label{lemma:L2-bound}
\end{lemma}
We will also use probabilistic distance to define notions of boundary. Take the $p$-boundary to consist of points at distance $\leq p$ from points of the opposite label.
\begin{defn}
For any $p > 0$, define $\partial_p = \{x \in \X: \dist(x, \X^{-s(x)}) \leq p\}$.
\label{defn:boundary}
\end{defn}
This includes all $x$ with $\eta(x) = 0$. Next, the $(p,q)$-boundary consists of points at distance $\leq q$ from the $p$-boundary.
\begin{defn}
For any $p,q > 0$, define $\partial_{p,q} = \{x \in \X: \dist(x, \partial_p) \leq q\}$.
\label{defn:boundary2}
\end{defn}

We can bound $\Delta_\ell$, the query region at level $\ell$, in terms of this second-order boundary.
\begin{lemma}
For any $0 \leq \ell \leq \lg (2n/k)$, we have $\Delta_\ell \subset \partial_{4/2^\ell, 2/2^\ell}$.
\label{lemma:delta-continuous}
\end{lemma}
Theorem~\ref{thm:label-complexity} thus still holds with $n \cdot \mu(\partial_{4/2^\ell, 2/2^\ell})$ in place of $|\Delta_\ell|$; see Theorem~\ref{thm:label-complexity-dist} in the Appendix. 

Using these notions, we are able to specialize the generic label complexity bound of Theorem~\ref{thm:label-complexity} to get rates under suitable margin conditions; this is given in Theorem~\ref{thm:label-complexity-specific} in the appendix. Here we just give a concrete scenario handled by the latter result; another such scenario, with smoothness and Tsybakov noise conditions, appears in the appendix.

\begin{figure}[t]
\begin{center}
\includegraphics[width=3in]{figures/curvature.pdf}
\end{center}
\caption{{\bf A 2D example with bounded curvature:} {\em Left:} We have $\eta = -0.2$ for the orange/red part of the domain and $\eta=0.2$ for the light and dark blue. The orange and light blue indicate the boundary sets for negative and positive respectively. {\em Right:} examples of the $L_1,L_2$ balls for different points. The green $L_1$ circles are the largest monochrome circle that contains the point. The purple $L_2$ circles are the union of all circles of half the radius or smaller. All of these circles are monochrome.}
\label{fig:curvature}
\end{figure}


\subsection{Example: Curvature and Massart noise}
\label{sec:Massart}

By way of example, consider a case where $\X \subset \R^d$ and $\mu$ admits a density. We take $\B$ to consist of all open balls $B(x,r) = \{z: \|z-x\| < r\}$ centered in $\X$, with $\B(x)$ being balls that contain $x$. By VC arguments, the size of $\B$ is effectively $O(n^{d+1})$; in practice, this can be reduced to a low-order polynomial using core-sets~\cite{BC03}.

We make two key assumptions: distribution $\mu$ on $\X$ is within a multiplicative factor of uniform; and all of $\X$ has $|\eta(x)| \geq \gamma$ except for a $(d-1)$-dimensional boundary of finite curvature.
\begin{itemize}[leftmargin=0.5cm]
\item (Strong density condition)
There are constants $c_o, c_1 > 0$ such that for all balls $B \in \B$,
we have $c_o \vol(B) \leq \mu(B) \leq c_1 \vol(B) $, where $\vol(\cdot)$ is $d$-dimensional volume.
\item (Massart noise and boundary condition) $\X = \X^+ \cup \X^- \cup \X^0$, where: $\eta(x) \geq \gamma$ throughout $\X^+$; $\eta(x) \leq -\gamma$ throughout $\X^-$; and $\X^0$ separates (intersects any line between) $\X^+$ and $\X^-$ and is a $(d-1)$-dimensional Riemannian manifold of reach $r_o > 0$.
\end{itemize}
The \emph{reach} condition says that any point at distance $< r_o$ of $\X^0$ has a unique nearest neighbor on $\X^0$. It is a common notion of curvature~\cite{F59,NSW06} and implies that $\X^+$ (resp., $\X^-$) can be covered by open balls of radius $r_o$ that do not touch $\X^- \cup \X^0$ (resp., $\X^+ \cup \X^0$). A 2D example obeying these conditions appears in Figure~\ref{fig:curvature}.
\begin{thm}
  Under the two conditions above, there are constants $c_2, c_3$ for
  which the following holds. Pick $0 < \delta < 1$ and take
  $k = O(((d \log n) + \log (1/\delta))/\gamma^2)$. If the algorithm
  of Fig.~\ref{alg:main} makes $c_2k \leq m \leq c_3 n^{(d-1)/d}$
  queries, it assigns Bayes-optimal labels to all but
  $C (k/m)^{1/(d-1)} + (2/n) \log (4/\delta)$ fraction of $X$.
\label{thm:massart-dist}
\end{thm}

 Here the constant $c_2$ includes a term $(1/r_o)^d$. The
$\tilde{O}(1/m^{1/(d-1)})$ rate of this theorem is an improvement over
the usual $1/m^{1/d}$ rate for random querying and was also observed
earlier, in a more restricted setting, using specialized
algorithms~\cite{CN08}.

\section{Analysis Details: distributional setting}

In the distributional setting, $X$ is drawn i.i.d.\ from a distribution $\mu$ on a topological space $\X$. We have a collection of regions $\B$.

\subsection{Sampling level and probability mass}

We start by showing how the fidelity of $\B$ (Definition~\ref{defn:conformity}) can be established.
\begin{lemma}
With probability at least $1-\delta$,
\begin{enumerate}
\item[(a)] For all $B \in \B$ with $n \mu(B) \geq 12 \ln (2|\B|/\delta)$, we have
$$ \frac{\mu(B)}{2} \leq \frac{|X_B|}{n} \leq 2 \mu(B) .$$
\item[(b)] For all $B \in \B$ with $n \mu(B) < 12 \ln (2|\B|/\delta)$, we have $|X_B| < 24 \ln (2 |\B|/\delta)$.
\end{enumerate}
\label{lemma:ball-size-bounds}
\end{lemma}
\begin{proof}
It is an immediate consequence of the multiplicative Chernoff bound that with probability at least $1-\delta$, for all $B \in \B$,
$$ \frac{|X_B|}{n} = \mu(B) \left(1 \pm \sqrt{\frac{3}{n \mu(B)} \ln \frac{2|\B|}{\delta}} \right) .$$
\end{proof}

In particular, taking $k > 24 \ln (2 |\B|/\delta)$ ensures that condition (a) hold will hold for all $B \in \B$ with $|X_B| \geq k$.

Next, we will see that balls of probability mass $p$ belong to level $\ell \approx \lg (1/p)$.
\begin{lemma}
For any level $\ell \geq 0$ and any $B \in \B_{\ell}$ with $|X_B| \geq k$,
$$ \left\lceil \lg \frac{1}{\mu(B)} \right\rceil -2 \leq \ell \leq  \left\lceil \lg \frac{1}{\mu(B)} \right\rceil.$$
\label{lemma:mass-vs-level}
\end{lemma}
\begin{proof}
Recall the definition of level $\ell$:
$$ B \in \B_\ell 
\ \Longleftrightarrow \ \frac{n}{2^{\ell+1}} \leq |X_B| < \frac{n}{2^\ell} 
\ \Longleftrightarrow \ 2^\ell < \frac{n}{|X_B|} \leq 2^{\ell + 1}.$$
By Lemma~\ref{lemma:ball-size-bounds}, 
$$ \frac{1}{2 \mu(B)} \leq \frac{n}{|X_B|} \leq \frac{2}{\mu(B)}.$$
Thus $2^{\ell} < 2/\mu(B)$ and $1/(2 \mu(B)) \leq 2^{\ell+1}$. These translate into the stated bounds on $\ell$.
\end{proof}

\subsection{Bounding $L_2$ using probabilistic distance: Proof of Lemma~\ref{lemma:L2-bound}}

Let $p = \dist(x, \X^{-s(x)}) \geq 2k/n$ and let $\ell = \lceil \lg (1/p) \rceil + 1 \leq \lg (2n/k)$. 

For any $B \in \B_{\geq \ell}(x)$, we have $\mu(B) \leq 2|X_B|/n < 2/2^\ell \leq p$, using fidelity (Definition~\ref{defn:conformity}), the definition of sampling levels, and the definition of $\ell$, in that order. It follows that $B$ does not intersect $\X^{-s(x)}$, whereupon $s(x) \cdot \eta_X(B) \geq 0$.

Next, pick any $B \in \B_{\leq \ell}(x)$ with $s(x) \cdot \eta_X(B) < 0$; thus $B \in \B_{< \ell}(x)$. By the richness of $\B$, there exists $B' \in \B_\ell(x)$ with $X_{B'} \subset X_B$; and so $s(x) \cdot \eta_X(B') \geq 0$. 
% Thus $B$ must intersect $\X^{-s(x)}$ and has probability mass $\geq p$. We will show that there exists $B' \in \B_{\leq \ell}(x)$ such that $B' \subset B$ and $B'$ does not intersect $\X^{-s(x)}$; whereupon $s(x) \cdot \eta_X(B') \geq 0$. Indeed, take $B' \in \B(x)$ to be a subset of $B$ of $\mu$-mass $p-\epsilon$ for some very small $\epsilon$; we can do this because of the absolute continuity of $\mu$. Then $B'$ does not touch $\X^{-s(x)}$ and by Lemma~\ref{lemma:mass-vs-level}, for small enough $\epsilon$, it lies at level $\leq \ell$.

\subsection{Uncertainty region in the distributional setting: Proof of Lemma~\ref{lemma:delta-continuous}}

Recall from (\ref{eq:sampling-region}) that
$$ \Delta_\ell = \bigcup_{x \in X: L_2(x) \geq \ell} \bigcup_{B \in \B_{\ell}(x)} X_B .$$
Consider any $z \in \Delta_\ell$. Then there exists $x \in \X$ with $L_2(x) \geq \ell$ and $B \in \B_\ell(x)$ such that $z \in B$. Now, $B \in \B_\ell(x)$ implies $|X_B|/n < 1/2^\ell$ and so (by the fidelity property) $\mu(B) < 2/2^\ell$. We will show that $x \in \partial_{4/2^\ell}$ and thus $z \in \partial_{4/2^\ell, 2/2^{\ell}}$.

There are two cases for $x$. If $\dist(x, \X^{-s(x)}) < 2k/n$ then we immediately have $x \in \partial_{4/2^\ell}$ since $4/2^\ell \geq 2k/n$. 

On the other hand, if $\dist(x, \X^{-s(x)}) \geq 2k/n$ then, since $L_2(x) \geq \ell$, we can apply Lemma~\ref{lemma:L2-bound} to get 
$$ \ell 
\ \leq \ \left\lceil \lg \frac{1}{\dist(x,\X^{-s(x)})} \right\rceil + 1 
\ \leq \ 
\lg \frac{1}{\dist(x,\X^{-s(x)})} + 2 .
$$
Thus $\dist(x, \X^{-s(x)}) \leq 1/2^{\ell-2}$ and $x \in \partial_{4/2^\ell}$.

\subsection{A generic label complexity bound in the distributional setting}

We will need to relate the size of the query region $\Delta_\ell$ to the probability mass of the corresponding second-order boundary. For this, we provide an analog of Lemma~\ref{lemma:level-distribution} for the distributional setting.
\begin{lemma}
With probability at least $1-2(\lg (n/k))e^{-k/3}$, the following hold for all levels $0 \leq \ell \leq \lg (n/4k)$.
\begin{enumerate}
\item[(a)] $|\{x \in X: T_x \leq \tau_\ell\}| \leq 2 n \tau_\ell$.
\item[(b)] $|\{x \in \Delta_{\ell}: T_x \leq \tau_\ell\}| \leq 2 n \mu(\partial_{4/2^\ell, 2/2^\ell}) \tau_\ell$.
\end{enumerate}
\label{lemma:level-distribution-dist}
\end{lemma}
\begin{proof} 
Part (a) is as in Lemma~\ref{lemma:level-distribution}. 

For (b), pick any subset $S \subset \X$. If $X$ consists of $n$ independent draws from $\mu$, then $|\{x \in S: T_x \leq \tau_\ell\}|$ has a $\mbox{binomial}(n, \mu(S) \tau_\ell)$ distribution with expectation $n \mu(S) \tau_\ell$. The probability that it is more than twice its expected value is, by a multiplicative Chernoff bound, at most $e^{-n \mu(S) \tau_\ell/3}$. We will apply this to the various sets $S = \partial_{4/2^\ell, 2/2^\ell}$ and take a union bound over them. In each application, we will also see that $n \mu(S) \tau_\ell \geq k$.

Pick any $\ell \leq \lg (n/4k)$. If $\partial_{4/2^\ell, 2/2^\ell} = \emptyset$, then the statement in (b) is trivially true given Lemma~\ref{lemma:delta-continuous}. So assume this is not the case. Writing $p = 4/2^\ell$, we need to check that $\mu(\partial_{p, p/2}) \tau_\ell \geq k/n$, or equivalently, $\mu(\partial_{p, p/2}) \geq \max(1/2^{\ell+2}, k/n) = 1/2^{\ell+2}$. Now, $\partial_{p,p/2} \neq \emptyset \Longrightarrow \partial_p \neq \emptyset$. Pick any $x \in \partial_p$. We have by the richness of $\B$ (Definition~\ref{defn:conformity}) that there exists $B \in \B_\ell(x)$. By the definition of level, $p/8 \leq |X_B|/n < p/4$ and thus, by the fidelity property, $p/16 \leq \mu(B) < p/2$. It follows that $B \subset \partial_{p,p/2}$, whereupon and $\mu(\partial_{p,p/2}) \geq \mu(B) \geq p/16 = 1/2^{\ell+2}$.
%By absolute continuity of $\mu$, we can grow a ball around $x$ of probability mass arbitrarily close to $p/2$, so that this ball is contained within $\partial_{p,p/2}$. Thus $\mu(\partial_{p,p/2}) \geq p/2-\epsilon$ for any $\epsilon > 0$. The required conditions then follow from the value of $p$.
\end{proof}

\begin{thm}
Suppose that $k \geq (192/\gamma^2) \ln (4 |\B|/\delta)$ and that the active learning algorithm makes $0 < m \leq n$ queries. Then with probability at least $1-3\delta$, all points $x \in X$ with $L_1(x) \leq \ell_1$ and $L_2(x) \leq \ell_2$ will get Bayes-optimal labels $\yh(x) = g^*(x)$, where 
$$ \ell_1 = \left\lfloor \lg \frac{m}{32k} \right\rfloor $$
and $\ell_2$ is the largest integer $\leq \lg (n/4k)$ such that
$$ \sum_{\ell = \ell_1 + 1}^{\ell_2} 2^\ell \mu(\partial_{4/2^\ell, 2/2^\ell}) \ < \ \frac{m}{32k} .$$
\label{thm:label-complexity-dist}
\end{thm}

\begin{proof}
The proof is identical to that of Theorem~\ref{thm:label-complexity-0}; the only change is to use Lemma~\ref{lemma:level-distribution-dist}(b) in place of Lemma~\ref{lemma:level-distribution}(b).
\end{proof}


\subsection{Rates of convergence under distributional conditions}

We now give label complexity bounds under three assumptions. Let $\X_\gamma = \{x \in \X: |\eta(x)| \geq \gamma\}$; thus $X \cap \X_\gamma$ is the set of points whose assigned labels will be judged. We further divide this set by label: for $s \in \{-1,+1\}$, let $\X^s_\gamma = \{x \in \X: s \cdot \eta(x) \geq \gamma\}$. 

The first assumption is on the curvature of the decision regions. It allows us to bound $L_1(x)$.
\begin{enumerate}
\item[(A1)] There is an absolute constant $p_o > 0$ for which the following holds: for any $x \in \X_\gamma$, there exists $B \in \B(x)$ such that $B \subset \X^{s(x)}_\gamma$ and $\mu(B) \geq p_o$.
\end{enumerate}
\begin{lemma}
Under (A1), every $x \in \X_\gamma$ has $L_1(x) \leq \lceil \lg (1/p_o) \rceil$, provided $n > k/2p_o$.
\label{lemma:L1-bound}
\end{lemma} 
\begin{proof}
Pick any $x \in X_\gamma$; apply (A1) to get $B \in \B(x)$ for which $\mu(B) \geq p_o$ and $B \subset \X^{s(x)}_\gamma$. By Lemma~\ref{lemma:mass-vs-level}, $B \in \B_\ell$ for $\ell \leq \lceil \lg (1/p_o) \rceil$. Now, $s(x) \cdot \eta_X(B) \geq \gamma$; moreover, for any $B' \in \B(x)$ with $X_{B'} \subset X_B$ we have $X_{B'} \subset \X^{s(x)}_\gamma$ and thus $s(x) \cdot \eta_X(B') \geq \gamma$ as well.
\end{proof}

The second assumption is a variant of the Tsybakov margin condition and bounds $|\Delta_\ell|$.
\begin{enumerate}
\item[(A2)] There are absolute constants $C > 0$ and $0 < \sigma < 1$ for which the following holds: for any $p > 0$, we have $\mu(\partial_{p,p}) \leq C p^\sigma$.
\end{enumerate}
If, for instance, $\mu$ were the uniform distribution over $[0,1]^d$, we would expect $\sigma \sim 1/d$.

The third assumption bounds the fraction of points with large $L_2$ values. It asks, what fraction of the $p$-boundary touches $\X_\gamma$? We consider two options: under (A3), the fraction is zero for $p$ small enough; under (A3'), the fraction is $p^\xi$.
\begin{enumerate}
\item[(A3)] There is an absolute constant $p_1 > 0$ such that $\mu(\partial_p \cap \X_\gamma) = 0$ for any $p < p_1$.
\item[(A3')] There are constants $C' > 0$ and $0 < \xi < 1$ such that $\mu(\partial_{p} \cap \X_\gamma) \leq C' p^\xi$ for any $p > 0$.
\end{enumerate}


Under these assumptions, we get rates of convergence as follows.
\begin{thm}
Assume (A1), (A2) and either (A3) or (A3'). There is an absolute constant $C''$ for which the following holds. Pick $0 < \delta < 1$ and take $k = O(((d \log n) + \log (1/\delta))/\gamma^2)$. If the algorithm of Fig.~\ref{alg:main} makes $m$ queries, for
$$ \frac{128k}{p_o} \ \leq \ m \ \leq \ \frac{64C \cdot 8^\sigma}{1-\sigma} \, n^{1-\sigma} k^\sigma,$$
then with probability at least $1-\delta$:
\begin{itemize}
\item Under (A3), all of $X \cap \X_\gamma$ get Bayes-optimal labels for $m > (512C/(1-\sigma)) \cdot (1/p_1)^{1-\sigma} \cdot k$.
\item Under (A3'),  Bayes-optimal labels get assigned to all but $ C'' (k/m)^{\xi/(1-\sigma)} + (2/n) \log (4/\delta)$
%$$ C'' \left(\frac{k}{m}\right)^{\xi/(1-\sigma)} + \frac{2 \log (4/\delta)}{n} $$
fraction of $X \cap \X_\gamma$.
\end{itemize}
\label{thm:label-complexity-specific}
\end{thm}
This theorem yields the result of Section~\ref{sec:Massart}. Another example is in Section~\ref{sec:Tsybakov}, where bounds are given under smoothness and Tsybakov margin conditions.

\begin{proof}
In this case, $\B$ is infinite, but by standard VC-dimension arguments there are only $O(n^{d+1})$ balls with distinct sets $X_B$. This governs the setting of $k$.

First, define $\ell_1 = \lfloor \lg (m/32k) \rfloor$ and observe that by Lemma~\ref{lemma:L1-bound}, all points in $\X_\gamma$ have 
$$ L_1(x) 
\ \leq \ 
\left\lceil \lg \frac{1}{p_o} \right\rceil
\ \leq \ 
\lg \frac{2}{p_o} 
\ \leq \ 
\lg \frac{m}{64k}
\ \leq \ 
\ell_1 .
$$
Next, pick  
$$ \ell_2 = \left\lfloor \frac{1}{1-\sigma} \left( \lg \frac{m}{32k} + \lg \frac{1-\sigma}{C \cdot 4^{1+\sigma}} \right) \right\rfloor.$$
The upper bound on $m$ ensures that this is at most $\lg (n/2k)$. Then, using (A2),
$$
\sum_{\ell = \ell_1+1}^{\ell_2} 2^\ell \mu(\partial_{4/2^\ell, 2/2^\ell})
\ \leq \ 
C \sum_{\ell = \ell_1+1}^{\ell_2} 2^\ell \left( \frac{4}{2^\ell} \right)^\sigma 
\ \leq \ 
C \cdot 4^\sigma \cdot \frac{2}{2^{1-\sigma}-1} \cdot 2^{(1-\sigma)\ell_2}
\ \leq \ 
\frac{m}{32k} .
$$
Applying Theorem~\ref{thm:label-complexity-dist}, we then see that all points in $\X_\gamma$ with $L_2(x) \leq \ell_2$ will be correctly classified. Any remaining point $x \in \X_\gamma$ has $L_2(x) > \ell_2$ and thus (by Lemma~\ref{lemma:L2-bound})
$$ \left\lceil \lg \frac{1}{\dist(x, \X^{-s(x)})} \right\rceil + 1 > \ell_2 
\ \Longrightarrow \ 
\dist(x, \X^{-s(x)}) < \frac{4}{2^{\ell_2}} \leq  \left( \frac{512C}{1-\sigma} \right)^{1/(1-\sigma)}  \cdot \left( \frac{k}{m} \right)^{1/(1-\sigma)} . 
$$
Call this quantity $p$; thus any such $x$ lies in $\partial_p$.

Under (A3), $\partial_p \cap \X_\gamma$ has zero probability mass for $p < p_1$, that is, if 
$$ \left( \frac{512C}{1-\sigma} \right)^{1/(1-\sigma)}  \cdot \left( \frac{k}{m} \right)^{1/(1-\sigma)} < p_1
\ \Longleftrightarrow \ 
m > \frac{512C}{1-\sigma} \cdot \frac{1}{p_1^{1-\sigma}} \cdot k .
$$

Under (A3'), $\mu(\partial_p \cap \X_\gamma) \leq C' p^{\xi}$; we can then apply a Bernstein bound to assert that with probability at least $1-\delta$, 
$$ |X \cap (\partial_p \cap \X_\gamma)| \ \leq \ \frac{3}{2} C' n p^{\xi} + 2 \log \frac{1}{\delta},$$
from which the bound in the theorem follows by defining $C''$ appropriately.
\end{proof}

\subsection{Label complexity under curvature and Massart noise: Proof of Theorem~\ref{thm:massart-dist}}

Let (C1) denote the strong density condition and (C2) the Massart noise and boundary condition. Theorem~\ref{thm:massart-dist} follows immediately from the more general result of Theorem~\ref{thm:label-complexity-specific}, once we establish how (C1) and (C2) relate to assumptions (A1), (A2), and (A3').

\begin{lemma}
Conditions (C1) and (C2) yield assumption (A1), with $p_o = r_o^d \cdot c_o \cdot v_d$, where $v_d$ is the volume of the unit ball in $\R^d$.
\label{lemma:A1}
\end{lemma}

\begin{proof}
Pick any $x \in \X_\gamma^+$ (the negative case is similar), and let $r = \inf_{z \in \X^0} \|x - z\|$. If $r \geq r_o$, then $B = B(x,r_o)$ is entirely in $\X_\gamma^+$. Otherwise, the reach condition (C2) implies the existence of a ball $B \subset \X_\gamma^+$ that contains $x$ and has radius $r_o$. Either way, $\mu(B) \geq c_o \vol(B) = c_o v_d r_o^d$ by (C1). 
\end{proof}

\begin{lemma}
Conditions (C1) and (C2) yield assumptions (A2) and (A3') with $\sigma = \xi = 1/d$. 
\end{lemma}

\begin{proof}
Under (C1), any ball of probability mass $\leq p$ has volume $\leq p/c_o$ and radius $\leq (p/(c_o v_d))^{1/d}$. Let's call this latter quantity $r$. Thus, any point in $\partial_p$ lies within distance $2r$ of the boundary, while a point in $\partial_{p,p}$ lies within distance $4r$. 

To bound the volume of $\partial_{p,p}$, we can associate each point in this region with its nearest neighbor in $\X^0$; by condition (C1), this projection map is uniquely defined for $r < r_o/4$. The volume of the region is thus $O(r)$ and under (C1), has probability mass $O(r) = O(p^{1/d})$.
\end{proof}


\subsection{Label complexity under smoothness and Tsybakov noise}
\label{sec:Tsybakov}

Now we specialize Theorem~\ref{thm:label-complexity-specific} to another situation, where the data distribution satisfies Holder smoothness and Tsybakov margin conditions.

Recall that $\dist(x,S)$ denotes the probability-distance from $x$ and set $S$. We will overload notation so that for $x,x' \in \X$,
$$ \dist(x,x') = \dist(x, \{x'\}) = \inf\{\mu(B): B \in \B(x) \cap \B(x')\},$$
that is, the probability mass of the smallest ball containing both $x$ and $x'$. We will impose the following conditions on the data.
\begin{enumerate}
\item[(C1)] [Strong density condition] This was introduced in the earlier (Massart) example.
\item[(C2')] [Holder-smoothness of conditional probability function] There exist constants $L, \alpha$ such that
$$ |\eta(x) - \eta(x')| \ \leq \ L \cdot \dist(x,x')^\alpha$$
for all $x,x' \in \X$.
\item[(C3')] [Tsybakov margin condition] There exists constants $M, \beta$ such that
$$ \mu(\{x \in \X: |\eta(x)| \leq \tau\}) \leq M \tau^\beta$$
for all $\tau \in (0,1)$.
\item[(C4')] [Bounded curvature] The boundaries $\{x \in \X: \eta(x) = \gamma\}$ and $\{x \in \X: \eta(x) = -\gamma\}$ are $(d-1)$-dimensional Riemannian manifolds of reach $r_o > 0$.
\end{enumerate}

Lemma~\ref{lemma:A1} continues to hold, with condition (C4') doing the job of (C2). This yields assumption (A1). For the remaining assumptions, we first obtain a consequence of the Holder condition.

\begin{lemma}
Under (C2'), for any $p, q > 0$,
\begin{enumerate}
\item[(a)] $x \in \partial_p \implies |\eta(x)| \leq L p^\alpha$.
\item[(b)] $x \in \partial_{p,q} \implies |\eta(x)| \leq L(p^\alpha + q^\alpha)$.
\end{enumerate}
\label{lemma:holder-consequence}
\end{lemma}
\begin{proof}
Pick any $x \in \partial_p$. Let $s(x) = \mbox{sign}(\eta(x))$. By definition of the $p$-boundary set, for any $\epsilon > 0$, there exists $x' \in \X^{-s(x)}$ such that $\dist(x,x') < p+\epsilon$. By the Holder condition, $|\eta(x) - \eta(x')| < L(p+\epsilon)^\alpha$ and thus $|\eta(x)| < L(p+\epsilon)^\alpha$. Since this holds for any $\epsilon > 0$, we get part (a).

For (b), pick $x \in \partial_{p,q}$ and $\epsilon > 0$. Then there exists $x' \in \partial_p$ with $\dist(x,x') < q+\epsilon$. As before, we use the Holder condition to conclude that $|\eta(x)| < |\eta(x')| + L(q+\epsilon)^\alpha$ and then invoke (a).
\end{proof}

\begin{lemma}
Conditions (C2') and (C3') yield assumptions (A2) and (A3) with $C = (2L)^\beta M$, $\sigma = \alpha \beta$, and $p_1 = (\gamma/L)^{1/\alpha}$.
\label{lemma:boundary-constants} 
\end{lemma}

\begin{proof}
By Lemma~\ref{lemma:holder-consequence}, $\partial_{p,p} \subset \{x \in \X: |\eta(x)| \leq 2Lp^\alpha\}$; the probability mass of this set can be bounded by (C3').

Also by Lemma~\ref{lemma:holder-consequence}, $\partial_p$ is entirely contained in $\{x \in \X: |\eta(x)| \leq L p^\alpha\}$. For $p < p_1$, this does not intersect $\X_\gamma$.
\end{proof}

\begin{thm}
Assume conditions (C1) and (C2')--(C4'). There are constants $c_2, c_3$ for which the following holds. Pick $0 < \delta < 1$ and take $k = O(((d \log n) + \log (1/\delta))/\gamma^2)$. Suppose the algorithm of Fig.~\ref{alg:main} makes $m$ queries, where
$$ \frac{c_2k}{p_1^{1-\sigma}} \ \leq \ m \ \leq \ c_3 n^{1-\sigma} ,$$
with $p_1, \sigma$ as given in Lemma~\ref{lemma:boundary-constants}. Then with probability at least $1-\delta$, all of $X \cap \X_\gamma$ is assigned Bayes-optimal labels.
\label{thm:smoothness-dist}
\end{thm}

In this setting, regular $k$-nearest neighbor classification would also correctly classify all of $X \cap X_\gamma$ if given $O(k/p_1)$ random labeled points. The benefit of active learning is to thus reduce the label complexity to $O(k/p_1^{1-\alpha\beta})$.
\end{document}




\section{FORMATTING INSTRUCTIONS}

To prepare a supplementary pdf file, we ask the authors to use \texttt{aistats2024.sty} as a style file and to follow the same formatting instructions as in the main paper.
The only difference is that the supplementary material must be in a \emph{single-column} format.
You can use \texttt{supplement.tex} in our starter pack as a starting point, or append the supplementary content to the main paper and split the final PDF into two separate files.

Note that reviewers are under no obligation to examine your supplementary material.

\section{MISSING PROOFS}

The supplementary materials may contain detailed proofs of the results that are missing in the main paper.

\subsection{Proof of Lemma 3}

\textit{In this section, we present the detailed proof of Lemma 3 and then [ ... ]}

\section{ADDITIONAL EXPERIMENTS}

If you have additional experimental results, you may include them in the supplementary materials.

\subsection{The Effect of Regularization Parameter}

\textit{Our algorithm depends on the regularization parameter $\lambda$. Figure 1 below illustrates the effect of this parameter on the performance of our algorithm. As we can see, [ ... ]}

\vfill

\end{document}



\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
